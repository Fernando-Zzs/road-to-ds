# 课时17

## 决策树工作原理

决策树是把我们以前的经验分类总结的模型。在做决策树的时候，会经历两个阶段：构造和剪枝。

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230201223209008.png" alt="image-20230201223209008" style="zoom:67%;" />

### 构造

构造时需要考虑三种节点（一个节点代表一种属性）：

- 根节点
- 内部节点
- 叶节点

需要解决三个重要问题：

- 选择哪个属性作为根节点
- 选择哪些属性作为子节点
- 什么时候停止并得到目标状态，即叶节点

### 剪枝

剪枝就是给决策树瘦身，避免出现**过拟合**现象。造成过拟合的原因之一就是因为训练集中**样本量较小**。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。

#### 预剪枝

在决策树构造时就进行剪枝。方法是在**构造的过程中**对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。

#### 后剪枝

在**生成决策树之后**再进行剪枝，通常会从决策树的叶节点开始，**逐层向上**对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。

方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。

## 如何选择决策树的节点

### 纯度

决策树构造过程就是寻找纯净划分的过程。一个集合中分歧最小的纯度即为最高。纯度也就是让目标变量的分歧最小。

### 信息熵（Entropy）

表示信息的不确定度，在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230201224102632.png" alt="image-20230201224102632" style="zoom:50%;" />

p(i|t) 代表了节点 t 为分类 i 的概率。

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230201224647325.png" alt="image-20230201224647325" style="zoom:50%;" />

它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。

信息熵越大，纯度越低。当集合中所有样本均匀混合时，信息熵最大，纯度最低。

### 不纯度的指标

我们在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。

#### ID3算法

##### 计算方法

计算的是信息增益，指的是划分可以带来纯度的提高，信息熵的下降。

它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的**归一化信息熵**，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230201224600016.png" alt="image-20230201224600016" style="zoom:50%;" />

公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230201224701944.png" alt="image-20230201224701944" style="zoom:50%;" />

所有的属性都计算一次信息增益，选择信息增益值最大的作为这一层的节点。

##### 优缺点分析

优点：ID3 的算法规则相对简单，可解释性强。

缺点：

- ID3 算法倾向于选择取值比较多的属性。有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。
- 噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。

#### C4.5算法

##### 计算方法

相比较ID3算法，此算法采用了信息增益率：

> 信息增益率=信息增益 / 属性熵

当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5来说，属性熵也会变大，所以整体的信息增益率并不大。

##### 优缺点分析

优点：

- 用信息增益率代替了信息增益，解决了噪声敏感的问题

- 采用悲观剪枝

  ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。
  悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。

- 离散化处理连续属性

  C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，C4.5 选择具有最高信息增益的划分所对应的阈值。

- 处理缺失值

  针对数据集不完整的情况，C4.5 也可以进行处理。当在计算某个属性的信息增益率时，如果某个样本的属性值缺失，可以先跳过，计算后的值再和`有效样本数/总样本数`相乘

缺点：需要对数据集进行多次扫描，算法效率相对较低。