# 课时17

## 决策树工作原理

决策树是把我们以前的经验分类总结的模型。在做决策树的时候，会经历两个阶段：构造和剪枝。

<img src=".\images\image-20230201223209008.png" alt="image-20230201223209008" style="zoom:67%;" />

### 构造

构造时需要考虑三种节点（一个节点代表一种属性）：

- 根节点
- 内部节点
- 叶节点

需要解决三个重要问题：

- 选择哪个属性作为根节点
- 选择哪些属性作为子节点
- 什么时候停止并得到目标状态，即叶节点

### 剪枝

剪枝就是给决策树瘦身，避免出现**过拟合**现象。造成过拟合的原因之一就是因为训练集中**样本量较小**。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。

#### 预剪枝

在决策树构造时就进行剪枝。方法是在**构造的过程中**对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。

#### 后剪枝

在**生成决策树之后**再进行剪枝，通常会从决策树的叶节点开始，**逐层向上**对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。

方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。

## 如何选择决策树的节点

### 纯度

决策树构造过程就是寻找纯净划分的过程。一个集合中分歧最小的纯度即为最高。纯度也就是让目标变量的分歧最小。

### 信息熵（Entropy）

表示信息的不确定度，在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：

<img src=".\images\image-20230201224102632.png" alt="image-20230201224102632" style="zoom:50%;" />

p(i|t) 代表了节点 t 为分类 i 的概率。

<img src=".\images\image-20230201224647325.png" alt="image-20230201224647325" style="zoom:50%;" />

它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。

信息熵越大，纯度越低。当集合中所有样本均匀混合时，信息熵最大，纯度最低。

### 不纯度的指标

我们在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。

#### ID3算法

##### 计算方法

计算的是信息增益，指的是划分可以带来纯度的提高，信息熵的下降。

它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的**归一化信息熵**，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：

<img src=".\images\image-20230201224600016.png" alt="image-20230201224600016" style="zoom:50%;" />

公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。

<img src=".\images\image-20230201224701944.png" alt="image-20230201224701944" style="zoom:50%;" />

所有的属性都计算一次信息增益，选择信息增益值最大的作为这一层的节点。

##### 优缺点分析

优点：ID3 的算法规则相对简单，可解释性强。

缺点：

- ID3 算法倾向于选择取值比较多的属性。有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。
- 噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。

#### C4.5算法

##### 计算方法

相比较ID3算法，此算法采用了信息增益率：

> 信息增益率=信息增益 / 属性熵

当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5来说，属性熵也会变大，所以整体的信息增益率并不大。

##### 优缺点分析

优点：

- 用信息增益率代替了信息增益，解决了噪声敏感的问题

- 采用悲观剪枝

  ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。
  悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。

- 离散化处理连续属性

  C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，C4.5 选择具有最高信息增益的划分所对应的阈值。

- 处理缺失值

  针对数据集不完整的情况，C4.5 也可以进行处理。当在计算某个属性的信息增益率时，如果某个样本的属性值缺失，可以先跳过，计算后的值再和`有效样本数/总样本数`相乘

缺点：需要对数据集进行多次扫描，算法效率相对较低。

# 课时18

## CART决策树

ID3和C4.5决策树可以生成二叉树或多叉树，CART只支持二叉树。CART决策树既可以作分类树，又可以作回归树。

分类树可以处理**离散**数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对**连续**型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。

### 分类树的工作原理

决策树的核心是寻找纯净的划分，在分类问题上，CART与C4.5类似，不同地方在于采用的指标是基尼系数。

基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。

<img src=".\images\image-20230202113838345.png" alt="image-20230202113838345" style="zoom:50%;" />

这里 p(Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。

在 CART 算法中，基于基尼系数对特征属性进行二元分裂，假设属性 A 将节点 D 划分成了D1 和 D2，如下图所示：

<img src=".\images\image-20230202113950898.png" alt="image-20230202113950898" style="zoom:50%;" />

节点 D 的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，用公式表示为：

<img src=".\images\image-20230202114011521.png" alt="image-20230202114011521" style="zoom:50%;" />

归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。

```python
# 创建 CART 分类树
clf = DecisionTreeClassifier(criterion='gini')
# 拟合构造 CART 分类树
clf = clf.fit(train_features, train_labels)
# 用 CART 分类树做预测
test_predict = clf.predict(test_features)
# 预测结果与测试集结果作比对
score = accuracy_score(test_labels, test_predict)
print("CART 分类树准确率 %.4lf" % score)
```



### 回归树的工作原理

CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。回归树使用的是样本的离散程度。

- 最小绝对偏差（LAD）

  <img src=".\images\image-20230202123422951.png" alt="image-20230202123422951" style="zoom:25%;" />

- 最小二乘偏差（LSD）

  <img src=".\images\image-20230202123459293.png" alt="image-20230202123459293" style="zoom: 50%;" />

```python
# 创建 CART 回归树
dtr=DecisionTreeRegressor()
# 拟合构造 CART 回归树
dtr.fit(train_features, train_price)
# 预测测试集中的房价
predict_price = dtr.predict(test_features)
# 测试集的结果评价
print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))
print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price)) 
```

### CART决策树的剪枝

#### Step1

采用cost-complexity prune（CPP，代价复杂度）方法，这是一种后剪枝方法，使用的指标是：**表面误差率增益值**。表示的是：节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。

<img src=".\images\image-20230202123823337.png" alt="image-20230202123823337" style="zoom:33%;" />

其中， C(t) 表示节点 t 的子树被剪枝后节点 t 的误差，Tt 代表以 t 为根节点的子树；C(Tt) 表示节点 t 的子树没被裁剪时子树 Tt 的误差；|Tt|代表子树 Tt 的叶子数，剪枝后，T 的叶子数减少了|Tt|-1。

我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。（重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。）

#### Step2

得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。

# 课时19

## 泰坦尼克号乘客生存预测

### 关键流程

<img src=".\images\image-20230202135644382.png" alt="image-20230202135644382" style="zoom:67%;" />

### 数据探索

如何进行数据探索呢？这里有一些函数你需要了解：

- 使用 info() 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；
- 使用 describe() 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；
- 使用 describe(include=[‘O’]) 查看字符串类型（非数字）的整体情况；
- 使用 head 查看前几行数据（默认是前 5 行）；
- 使用 tail 查看后几行数据（默认是最后 5 行）。

### 清洗数据

对于缺失的数据，考虑使用平均值、最高频值的方式补全，当缺失值过多时放弃补全。

其他情况参照前面所讲的准则进行数据清洗。

### 特征选择

特征选择是分类器的关键。特征选择不同，得到的分类器也不同。那么我们该选择哪些特征做生存的预测呢？这是基于常识和经验的工作，判断哪些列可能会与预测分类有关，提取出来作为训练的特征。

当特征值不是数值类型时，可以使用DictVectorizer类进行转化，这个过程会改变特征值的数量。比如Sex列会拆分成“Sex=female”和“Sex=male”两列，结果用0/1表示。

```python
from sklearn.feature_extraction import DictVectorizer
dvec=DictVectorizer(sparse=False)
train_features=dvec.fit_transform(train_features.to_dict(orient='record'))
```

这里用到fit_transform，是fit和transform的组合，既包括了训练又包含了转换。

transform()和fit_transform()二者的功能都是对数据进行某种统一处理（比如标准化~N(0,1)，将数据缩放(映射)到某个固定区间，归一化，正则化等）

不同点在于fit_transform(trainData)对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等。

### 决策树模型

目前为止，sklearn 中只实现了 ID3 与 CART 决策树，所以我们暂时只能使用这两种决策树，在构造DecisionTreeClassifier 类时，其中有一个参数是 criterion，意为标准。它决定了构造的分类树是采用 ID3 分类树，还是 CART 分类树，对应的取值分别是 entropy或者 gini。

除了设置 criterion 采用不同的决策树算法外，一般建议使用默认的参数，默认参数不会限制决策树的最大深度，不限制叶子节点数，认为所有分类的权重都相等等。当然你也可以调整这些参数，来创建不同的决策树模型。

在构造决策树分类器后，我们可以使用 fit 方法让分类器进行拟合。

### 模型评估与预测

使用 predict 方法对新数据进行预测，得到预测的分类结果，也可以使用 score 方法得到分类器的准确率。模型准确率需要考虑是否有测试集的实际结果可以做对比，当测试集没有真实结果可以对比时，需要使用 **K 折交叉验证cross_val_score **的方法。

#### K折交叉验证

使用 K 折交叉验证的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。K 折交叉验证，就是做 K 次交叉验证，每次选取 K 分之一的数据作为验证，其余作为训练。轮流 K 次，取平均值。K 折交叉验证的原理是这样的：
1. 将数据集平均分割成 K 个等份；
2. 使用 1 份数据作为测试数据，其余作为训练数据；
3. 计算测试准确率；
4. 使用不同的测试集，重复 2、3 步骤。

### 决策树可视化

使用 Graphviz 可视化工具帮我们把决策树呈现出来。 

# 课时20

## 贝叶斯原理

### 逆向概率

所谓“逆向概率”是相对“正向概率”而言。正向概率即了解事情全貌，以上帝视角来判断一件事发生的概率是多少；逆向概率则是建立在主观判断的基础上，在我们不了解所有客观事实的情况下，先估计一个值，然后根据实际结果不断进行修正。

### 先验概率

通过经验来判断事情发生的概率，就是先验概率。

### 后验概率

发生结果之后推测原因的概率，属于条件概率的一种。

### 条件概率

事件A在另一个事件B已经发生的条件下发生的概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。

### 似然函数

你可以把概率模型的训练过程理解为求参数估计的过程。举个例子，如果一个硬币在 10 次抛落中正面均朝上。那么你肯定在想，这个硬币是均匀的可能性是多少？这里硬币均匀就是个参数，似然函数就是用来衡量这个模型的参数。似然在这里就是可能性的意思，它是关于统计参数的函数。

### 贝叶斯原理

贝叶斯原理就是求解后验概率，通常我们知道先验概率P(Bi)，知道观测值在不同类别发生情况下的条件概率P(Bi|A)，就可以求出观测值的情况下不同类别发生的后验概率：

<img src=".\images\image-20230202204028518.png" alt="image-20230202204028518" style="zoom:50%;" />

## 朴素贝叶斯

简单、强大的预测建模算法，它假设每个输入变量是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然非常有效。

朴素贝叶斯模型由两种类型的概率组成：
1. 每个**类别的概率**P(Cj)
2. 每个属性的**条件概率**P(Ai|Cj)

为了训练朴素贝叶斯模型，我们需要先给出训练数据，以及这些数据对应的分类。那么上面这两个概率，也就是类别概率和条件概率。他们都可以从给出的训练数据中计算出来。一旦计算出来，概率模型就可以使用贝叶斯原理对新数据进行预测。

## 概念区别

贝叶斯原理、贝叶斯分类和朴素贝叶斯这三者之间是有区别的。

贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。不过好在对于大部分情况下，朴素贝叶斯的分类效果都不错。

## 朴素贝叶斯分类的工作原理

朴素贝叶斯分类是常用的贝叶斯分类方法。我们看到一个陌生人，要做的第一件事情就是判断 TA 的性别，判断性别的过程就是一个分类的过程。根据以往的经验，我们通常会从身高、体重、鞋码、头发长短、服饰、声音等角度进行判断。这里的“经验”就是一个训练好的关于性别判断的模型，其训练数据是日常中遇到的各式各样的人，以及这些人实际的性别数据。

### 离散数据案例

#### 问题描述

<img src=".\images\image-20230202205549457.png" alt="image-20230202205549457" style="zoom:50%;" />

给定上面的数据，现在有一个新的数据：身
高“高”、体重“中”，鞋码“中”，请问这个人是男还是女？

#### 推导过程

##### Step1

属性：3个，分别用A1, A2, A3 代表身高 = 高、体重 = 中、鞋码 = 中

类别：2个，C1,C2 分别是：男、女

##### Step2

<img src=".\images\image-20230202205756497.png" alt="image-20230202205756497" style="zoom: 33%;" />

因为一共有 2 个类别，所以我们只需要求得 P(C1|A1A2A3) 和 P(C2|A1A2A3) 的概率即可，然后比较下哪个分类的可能性大，就是哪个分类结果。

因为 P(A1A2A3) 都是固定的，我们想要寻找使得 P(Cj|A1A2A3) 的最大值，就等价于求 P(A1A2A3|Cj)P(Cj) 最大值。

##### Step3

我们假定 Ai 之间是相互独立的，那么：
P(A1A2A3|Cj)=P(A1|Cj)P(A2|Cj)P(A3|Cj)

然后我们需要从 Ai 和 Cj 中计算出 P(Ai|Cj) 的概率，带入到上面的公式得出P(A1A2A3|Cj)，最后找到使得 P(A1A2A3|Cj) 最大的类别 Cj。

### 连续数据案例

#### 问题描述

<img src=".\images\image-20230202210232698.png" alt="image-20230202210232698" style="zoom:50%;" />

如果给你一个新的数据，身高 180、体重 120，鞋码 41，请问该人是男是女呢？

#### 推导过程

##### Step1

将属性假设为正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。这些计算可以通过 EXCEL 的NORMDIST(x,mean,standard_dev,cumulative) 函数实现，一共有 4 个参数：
1. x：正态分布中，需要计算的数值；
2. Mean：正态分布的平均值；
3. Standard_dev：正态分布的标准差；
4. Cumulative：取值为逻辑值，即 False 或 True。它决定了函数的形式。当为 TRUE
时，函数结果为累积分布；为 False 时，函数结果为概率密度。

##### Step2

与离散案例一样，分别计算各个类别下的P(A1A2A3|Ci)=P(A1|Ci)P(A2|Ci)P(A3|Ci)，取最大值即为分类结果。

## 朴素贝叶斯的工作流程

<img src=".\images\image-20230202210701922.png" alt="image-20230202210701922" style="zoom:67%;" />

第一阶段：人工完成的阶段，对每个特征属性进行适当划分，对一部分数据进行分类，形成训练样本；

第二阶段：生成分类器，计算每个类别在训练样本中的出现频率、每个特征属性划分在每个类别情况下的条件概率，输出分类器。

第三阶段：输入新数据，得到预测的分类结果。

# 课时21

## 朴素贝叶斯的应用

朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理 NLP 的工具。

## sklearn机器学习包

sklearn 的全称叫 Scikit-learn，它给我们提供了 3 个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。

这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：

- 高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。
- 多项式朴素贝叶斯：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。
- 伯努利朴素贝叶斯：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。

## TF-IDF值

TF-IDF 是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。实际上是词频和逆向文档频率两者的乘积，倾向于找到 TF 和IDF 取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。

### TF

词频（Term Frequency）计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。

### IDF

逆向文档频率（Inverse Document Frequency），是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。**IDF 越大就代表该单词的区分度越大。**

<img src=".\images\image-20230202211710598.png" alt="image-20230202211710598" style="zoom:50%;" />

为什么 IDF 的分母中，单词出现的文档数要加 1 呢？因为有些单词可能不会存在文档中，为了避免分母为 0，统一给单词出现的文档数都加 1。

## 如何计算TF-IDF值

通过TfidfVectorizer 类

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# TfidfVectorizer 类的构造函数
# TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)

tfidf_vec = TfidfVectorizer()
documents = [
 'this is the bayes document',
 'this is the second second document',
 'and the third one',
 'is this the document'
]
tfidf_matrix = tfidf_vec.fit_transform(documents) # 进行模型的拟合，可以返回每个单词在每个文档中的TF-IDF值

print('不重复的词:', tfidf_vec.get_feature_names())
print('每个单词的 ID:', tfidf_vec.vocabulary_)
print('每个单词的 tfidf 值:', tfidf_matrix.toarray())
```

- stop_words表示自定义停用词，输入一个List，包含在分类中没有用的词（TF高IDF很低），这么做可以节省空间和计算时间。
- token_pattern表示过滤规则，输入正则表达式

## 如何对文档进行分类

<img src=".\images\image-20230202213100549.png" alt="image-20230202213100549" style="zoom:67%;" />

### 文档分词

英文文档可以使用NTLK包进行分词，包含了停用词、分词和标注方法等功能；在中文文档中，最常用的是 jieba 包。jieba 包中包含了中文的停用词 stop words 和分词方法。

```python
import nltk
word_list = nltk.word_tokenize(text) # 分词
nltk.pos_tag(word_list) # 标注单词的词性

import jieba
word_list = jieba.cut(text) # 中文分词
```

### 加载停用词表

我们需要自己读取停用词表文件，从网上可以找到中文常用的停用词保存在stop_words.txt，然后利用 Python 的文件读取函数读取文件，保存在 stop_words 数组中。

```python
stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readline()]
```

### 计算单词的权重

也就是计算TF-IDF特征空间features，选出来的分词就是特征。

```python
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5) # max_df参数用来描述单词在文档中的最高出现率阈值，如果超过这个值则不作为分词统计
features = tf.fit_transform(train_contents)
```

### 生成朴素贝叶斯分类器

```python
# 多项式贝叶斯分类器
from sklearn.naive_bayes import MultinomialNB 
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)
```

我们将特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 传递给贝叶斯分类器 clf，它会自动生成一个符合特征空间和对应分类的分类器。

这里我们采用的是多项式贝叶斯分类器，其中 alpha 为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。为了解决这个问题，我们需要做平滑处理。

- 当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。
- 当 0<alpha<1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。

### 用分类器做预测

```python
# 得到测试集的特征矩阵
test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)
# 得到测试集的特征矩阵 test_features
test_features=test_tf.fit_transform(test_contents)
# 传入测试集的特征矩阵，得到分类结果
predicted_labels=clf.predict(test_features)
```

### 计算准确率

在模型评估中，sklearn 提供了 metrics 包，帮我们对预测结果与实际结果进行评估。

```python
from sklearn import metrics
print metrics.accuracy_score(test_labels, predicted_labels)
```

# 课时22

## SVM

中文名为支持向量机，是一种有监督的学习模型，即我们需要事先对数据打上分类标签，这样机器就知道这个数据属于哪个分类。

无监督学习就是数据没有被打上分类标签，这可能是因为我们不具备先验的知识，或者打标签的成本很高。所以我们需要机器代我们部分完成这个工作，比如将数据进行聚类，方便后续人工对每个类进行分析。

<img src=".\images\image-20230203112730330.png" alt="image-20230203112730330" style="zoom: 33%;" />

我们要分开两种颜色的小球，除非用曲线，否则很难分得开，如果我们将二维空间升维为三维空间，原来的曲线变成了一个平面，我们就说这个平面为超平面。

<img src=".\images\image-20230203112905788.png" alt="image-20230203112905788" style="zoom: 33%;" />

## SVM的工作原理

**SVM 就是帮我们找到一个超平面**，这个超平面能将不同的样本划分开，同时使得样本集中的点到这个分类超平面的最小距离（即分类间隔）最大化。

如下图三根直线都可以分开两种颜色小球，但C的鲁棒性更强，我们要找到C这样的最优解。

<img src=".\images\image-20230203113011405.png" alt="image-20230203113011405" style="zoom:33%;" />

### 分类间隔margin

在多维空间中，有一个平面可以作为分隔平面，我们移动该平面知道产生两个极限位置A和B，再取中间值即为最优决策面C。

如果我们转动这个最优决策面，你会发现可能存在多个最优决策面，它们都能把数据集正确分开，这些最优决策面的分类间隔可能是不同的，而那个拥有“最大间隔”（max margin）的决策面就是 SVM 要找的最优解。

### 点到超平面的距离公式

在高维空间中，我们可以把超平面表示为：

<img src=".\images\image-20230203113406482.png" alt="image-20230203113406482" style="zoom:33%;" />

在这个公式里，w、x 是 n 维空间里的向量，其中 x 是函数变量；w 是法向量。法向量这里指的是垂直于平面的直线所表示的向量，它决定了超平面的方向。

**支持向量**就是离**分类超平面**最近的样本点，实际上如果确定了支持向量也就确定了这个超平面。所以支持向量决定了分类间隔到底是多少，而在最大间隔以外的样本点，其实对分类都没有意义。问题就转变成**如何求解最大分类间隔**。

我们定义某类样本集到超平面的距离是这个样本集合内的样本到超平面的最短距离。我们用 di 代表点 xi 到超平面 wxi+b=0 的欧氏距离。因此我们要求 di 的最小值，用它来代表这个样本到超平面的最短距离。di 可以用公式计算得出：

<img src=".\images\image-20230203113651495.png" alt="image-20230203113651495" style="zoom:33%;" />

其中||w||为超平面的范数，di 的公式可以用解析几何知识进行推导，这里不做解释。在数学上，这是一个凸优化问题（凸优化就是关于求凸集中的凸函数最小化的问题，这里不具体展开）。通过凸优化问题，最后可以求出最优的 w 和 b，也就是我们想要找的最优超平面。中间求解的过程会用到拉格朗日乘子，和 KKT（Karush-Kuhn-Tucker）条件。

## 硬间隔、软间隔和非线性 SVM

硬间隔指的就是完全分类准确，不能存在分类错误的情况。

软间隔就是允许一定量的样本分类错误。

<img src=".\images\image-20230203113903403.png" alt="image-20230203113903403" style="zoom:33%;" />

非线性SVM无论多高级的分类器，只要映射函数是线性的，就没法处理，SVM 也处理不了。这时，我们需要引入一个新的概念：**核函数（它可以将样本从原始空间映射到一个更高维的特质空间中，使得样本在新的空间中线性可分）**。这样我们就可以使用原来的推导来进行计算，只是所有的推导是在新的空间，而不是在原来的空间中进行。

<img src=".\images\image-20230203114017463.png" alt="image-20230203114017463" style="zoom:33%;" />

## 解决多分类问题

SVM本身是二值分类器，如果要解决多分类问题，效率一般不会高。有以下两种方法：

### 一对多法

假设我们要把物体分成 A、B、C、D 四种分类，那么我们可以先把其中的一类作为分类 1，其他类统一归为分类 2。这样我们可以构造 4 种 SVM，分别为以下的情况：

> （1）样本 A 作为正集，B，C，D 作为负集；
>
> （2）样本 B 作为正集，A，C，D 作为负集；
>
> （3）样本 C 作为正集，A，B，D 作为负集；
>
> （4）样本 D 作为正集，A，B，C 作为负集。

优点：分类速度较快

缺点：针对 K 个分类，需要训练 K 个分类器，训练速度较慢，因为每个分类器都需要对全部样本进行训练，而且负样本数量远大于正样本数量，会造成样本不对称的情况，而且当增加新的分类，比如第 K+1 类时，需要重新对分类器进行构造。

### 一对一法

我们可以在任意两类样本之间构造一个 SVM，这样针对 K 类的样本，就会有 C(k,2) 类分类器。当对一个未知样本进行分类时，每一个分类器都会有一个分类结果，即为 1 票，最终得票最多的类别就是整个未知样本的类别。

比如我们想要划分 A、B、C 三个类，可以构造 3 个分类器：

> （1）分类器1： A、B；
>
> （2）分类器2： A、C；
>
> （3）分类器3： B、C。

优点：如果新增一类，不需要重新训练所有的 SVM，只需要训练和新增这一类样本的分类器。而且这种方式在训练单个 SVM 模型的时候，训练速度快。

缺点：分类器的个数与 K 的平方成正比，所以当 K 较大时，训练和测试的时间会比较慢。

# 课时23

## SVM实战

SVM 既可以做回归，也可以做分类器。

当用 SVM 做回归的时候，我们可以使用 SVR(Support Vector Regression) 或 LinearSVR。当做分类器的时候，我们使用的是 SVC(Support Vector Classification) 或者 LinearSVC。

如果处理线性可分的数据，选择LinearSVC，在 LinearSVC 中没有 kernel 这个参数，限制我们只能使用线性核函数。由于 LinearSVC 对线性分类做了优化，对于数据量大的线性可分问题，使用 LinearSVC 的效率要高于 SVC；

如果是针对非线性的数据，需要用到 SVC。在 SVC 中，我们既可以使用到线性核函数（进行线性划分），也能使用高维的核函数（进行非线性划分）。

```python
model = svm.SVC(kernel='rbf', C=1.0, gamma='auto')
model.fit(train_X,train_y)
model.predict(test_X)
```

### 参数说明

kernel 代表核函数的选择：

1. linear：线性核函数。是在数据线性可分的情况下使用的，运算速度快，效果好。不足在于它不能处理线性不可分的数据。
2. poly：多项式核函数。可以将数据从低维空间映射到高维空间，但参数比较多，计算量大。
3. rbf：高斯核函数（默认）。将样本映射到高维空间，但相比于多项式核函数来说所需的参数比较少，通常性能不错，所以是默认使用的核函数。
4. sigmoid：sigmoid 核函数。经常用在神经网络的映射中。因此当选用 sigmoid 核函数时，SVM 实现的是多层神经网络。

参数 C 代表目标函数的**惩罚系数**，惩罚系数指的是分错样本时的惩罚程度，默认情况下为 1.0。当 C 越大的时候，分类器的准确性越高，但同样容错率会越低，泛化能力会变差。相反，C 越小，泛化能力越强，但是准确性会降低。

参数 gamma 代表核函数的系数，默认为样本特征数的倒数，即 gamma = 1 / n_features。

## 乳腺癌案例实战

[代码](.\learnDS\breast_svm.py)

# 课时24

## KNN的原理

简单来说就是，近朱者赤近墨者黑。KNN可以应用到线性和非线性的分类问题中，也可以用于回归分析。

### 分类问题

1. 计算待分类物体和其他物体之间的距离
2. 统计距离最近的K个邻居
3. K个邻居中属于哪个类别的多，则将该物体分为该类别

### 回归问题

1. 对于一个新点，找出K个最近邻居
2. 将这些邻居的属性平均值赋给该点，得到该点的属性

这里邻居的影响力权重是可以设置的。

## K值如何选择

K值是实践得出的结果，一般采用交叉验证的方法选取K值，一般会把K值选取在较小的范围内，同时在验证集上准确率最高的哪一个最终确定为K值。

K值过小：容易产生过拟合，受到噪声影响大。

K值过大：鲁棒性强，但容易产生欠拟合，距离过远的点也会对未知物体的分类产生影响。

## 距离如何计算

关于距离的计算方式有下面五种方式（前三种是常用的）：

对于两个点(x1,x2,...,xn)和(y1,y2,...,yn)

1. 欧氏距离

   在n维空间中，就是两个向量之间对应维度做差的平方和开根号

   <img src=".\images\image-20230203165338722.png" alt="image-20230203165338722" style="zoom:50%;" />

2. 曼哈顿距离

   在几何空间中用的比较多，简单说就是两个点在坐标系上的绝对轴距总和

   <img src=".\images\image-20230203165539071.png" alt="image-20230203165539071" style="zoom:33%;" />

3. 闵可夫斯基距离

   不是一个距离而是一组距离的定义，p代表空间的维数，当 p=1 时，就是曼哈顿距离；当 p=2 时，就是欧氏距离；当 p→∞时，就是切比雪夫距离。

   <img src=".\images\image-20230203165655274.png" alt="image-20230203165655274" style="zoom:25%;" />

4. 切比雪夫距离

   二个点之间的切比雪夫距离就是这两个点坐标数值差的绝对值的最大值，用数学表示就是：max(|x1-y1|,|x2-y2|)

5. 余弦距离

   实际上计算的是两个向量的夹角，是在方向上计算两者之间的差异，对绝对数值不敏感。在兴趣相关性比较上，角度关系比距离的绝对值更重要，因此余弦距离可以用于衡量用户对内容兴趣的区分度。

## KD树

KNN 的计算过程是大量计算样本点之间的距离。为了减少计算距离次数，提升 KNN 的搜索效率，人们提出了 KD 树（K-Dimensional 的缩写），可以很方便地存储 K 维空间的数据。KD 树是对数据点在 K 维空间中划分的一种数据结构。在 KD 树的构造中，每个节点都是 k 维数值点的二叉树。既然是二叉树，就可以采用二叉树的增删改查操作，这样就大大提升了搜索效率。

# 课时25

## KNN实战

### 调包

```python
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import KNeighborsRegressor
```

### 构造函数

```python
KNeighborsClassifier(n_neighbors=5, weights=‘uniform’, algorithm=‘auto’, leaf_size=30)
```

1. n_neighbors：即 KNN 中的 K 值，代表的是邻居的数量。K 值如果比较小，会造成过拟合。如果 K 值比较大，无法将未知物体分类出来。一般我们使用默认值 5。
2. weights：是用来确定邻居的权重，有三种方式：
   - weights=uniform，代表所有邻居的权重相同；
   - weights=distance，代表权重是距离的倒数，即与距离成反比；
   - 自定义函数，你可以自定义不同距离所对应的权重。大部分情况下不需要自己定义函数。

3. algorithm：用来规定计算邻居的方法，它有四种方式：
   - algorithm=auto，根据数据的情况自动选择适合的算法，默认情况选择 auto；
   - algorithm=kd_tree，也叫作 KD 树，是多维空间的数据结构，方便对关键数据进行检索，不过 KD 树适用于维度少的情况，一般维数不超过 20，如果维数大于 20 之后，效率反而会下降；
   - algorithm=ball_tree，也叫作球树，它和 KD 树一样都是多维空间的数据结果，不同于 KD 树，球树更适用于维度大的情况；
   - algorithm=brute，也叫作暴力搜索，它和 KD 树不同的地方是在于采用的是线性扫描，而不是通过构造树结构进行快速检索。当训练集大的时候，效率很低。

4. leaf_size：代表构造 KD 树或球树时的叶子数，默认是 30，调整 leaf_size 会影响到树的构造和搜索速度。

## MNIST手写数字集识别分类实战

[代码](.\learnDS\knn.py)

# 课时26

## K-Means工作原理

是一种非监督学习，解决的是聚类问题，本质是确定K类的中心点。

1. 选取 K 个点作为**初始的类中心点**，这些点一般都是从数据集中随机抽取的；
2. 将每个点**分配**到最近的类中心点，这样就形成了 K 个类，然后**重新计算每个类的中心点**；
3. 重复第二步，直到类不发生变化，或者你也可以设置最大迭代次数，这样即使类中心点发生变化，但是只要达到最大迭代次数就会结束。

### 选错了中心点怎么办

K-Means 有自我纠正机制，在不断的迭代过程中，会纠正中心点。中心点在整个迭代过程中，并不是唯一的，只是你需要一个初始值，一般算法会随机设置初始的中心点。

### 如何分配

距离计算方式同样有多种，经常选用欧氏距离。

- 欧氏距离
- 曼哈顿距离
- 切比雪夫距离
- 余弦距离

### 如何更新中心点

取一个类中的所有点取平均值作为新的中心点，然后根据此点按照距离远近重新分配 

## KMeans实战

### 构造函数

```python
from sklearn.cluster import KMeans
KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=300, tol=0.0001, precompute_distances='auto', verbose=0, random_state=None, copy_x=True, n_jobs=1, algorithm='auto')
```

我们能看到在 K-Means 类创建的过程中，有一些主要的参数：

- **n_clusters**: 即 K 值，一般需要多试一些 K 值来保证更好的聚类效果。你可以随机设置一些 K 值，然后选择聚类效果最好的作为最终的 K 值；
- **max_iter**： 最大迭代次数，如果聚类很难收敛的话，设置最大迭代次数可以让我们及时得到反馈结果，否则程序运行时间会非常长；
- **n_init**：初始化中心点的运算次数，默认是 10。程序是否能快速收敛和中心点的选择关系非常大，所以在中心点选择上多花一些时间，来争取整体时间上的快速收敛还是非常值得的。由于每一次中心点都是随机生成的，这样得到的结果就有好有坏，非常不确定，所以要运行 n_init 次, 取其中最好的作为初始的中心点。如果 K 值比较大的时候，你可以适当增大 n_init 这个值；
- **init：** 即初始值选择的方式，默认是采用优化过的 k-means++ 方式，你也可以自己指定中心点，或者采用 random 完全随机的方式。自己设置中心点一般是对于个性化的数据进行设置，很少采用。random 的方式则是完全随机的方式，一般推荐采用优化过的 k-means++ 方式；
- **algorithm**：k-means 的实现算法，有“auto” “full”“elkan”三种。一般来说建议直接用默认的"auto"。简单说下这三个取值的区别，如果你选择"full"采用的是传统的 K-Means 算法，“auto”会根据数据的特点自动选择是选择“full”还是“elkan”。我们一般选择默认的取值，即“auto” 。

### 功能调用

```python
kmeans.fit(train_x)
predict_y = kmeans.predict(train_x)
```

## K-Means与KNN的区别

- K-Means是聚类算法，KNN是分类算法
- K-Means是非监督学习，不需要事先给出分类标签；KNN是有监督学习，需要我们给出训练数据的分类标识
- K的含义不同，KMeans代表K类，KNN代表的是最近的K个邻居

# 课时27

## K-Means图像分割实战

[代码](.\learnDS\kmeans1.py)

# 课时28

## EM聚类

EM聚类算法也叫最大期望算法，英文是Expectation Maximization。可以理解为一种聚类框架，包含了Expectation步骤和Maximization步骤。

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230204161700921.png" alt="image-20230204161700921" style="zoom: 50%;" />

## 工作原理

### 最大似然估计

指的是一件事情已经发生了，然后反推更有可能是什么因素造成的，是一种通过已知结果估计参数的方法。

EM算法是一种求解最大似然估计的方法，通过观测样本，来找出样本的模型参数。

### 步骤

假设我们要投掷一枚硬币，有A、B两枚硬币，共做5次实验，每组实验投掷10次，统计出现正面的次数

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230204162154914.png" alt="image-20230204162154914" style="zoom:67%;" />

在这个案例里，我们可以

1. 先初始化参数，即A和B的正面概率（随即指定）
2. 然后计算期望值，两枚硬币在每次实验正面次数的概率，得出实验更有可能的硬币
3. 根据猜测的结果来完善初始化的参数

重复第二步和第三步，直至参数不再发生变化。

> 简单总结下上面的步骤，你能看出 EM 算法中的 E 步骤就是通过旧的参数来计算隐藏变量。然后在 M 步骤中，通过得到的隐藏变量的结果来重新估计参数。直到参数不再发生变化，得到我们想要的结果。

## 与K-Means的区别

相比于K-Means算法，EM聚类更加灵活，是一种软聚类算法。

K-Means算法是通过距离来区分样本之间差别的，且每个样本在计算时只能属于一个分类，称之为硬聚类算法。而 EM 聚类在求解的过程中，实际上每个样本都有一定的概率和每个聚类相关，叫做软聚类算法。

K-Means算法聚类结果：

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230204170625309.png" alt="image-20230204170625309" style="zoom:67%;" />

EM聚类结果：

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230204170940738.png" alt="image-20230204170940738" style="zoom:67%;" />

EM算法可以理解为一个框架，可以用不同的模型来求解。常用的 EM 聚类有 GMM 高斯混合模型和 HMM 隐马尔科夫模型。但核心还是E步和M步：E 步相当于通过初始化的参数来估计隐含变量，M 步是通过隐含变量来反推优化参数。最后通过 EM 步骤的迭代得到最终的模型参数。

# 课时29

## EM聚类实战

### 构造函数

```python
# 调包，以GMM为例
from sklearn.mixture import GaussianMixture
gmm = GaussianMixture(n_components=1, covariance_type=‘full’, max_iter=100)
```

1. n_components：即高斯混合模型的个数，也就是我们要聚类的个数，默认值为 1。如果你不指定 n_components，最终的聚类结果都会为同一个值。
2. covariance_type：代表协方差类型。一个高斯混合模型的分布是由均值向量和协方差矩阵决定的，所以协方差的类型也代表了不同的高斯混合模型的特征。协方差类型有 4 种取值：
   * covariance_type=full，代表完全协方差，也就是元素都不为 0；
   * covariance_type=tied，代表相同的完全协方差；
   * covariance_type=diag，代表对角协方差，也就是对角不为 0，其余为 0；
   * covariance_type=spherical，代表球面协方差，非对角为 0，对角完全相同，呈现球面的特性。

3. max_iter：代表最大迭代次数，EM 算法是由 E 步和 M 步迭代求得最终的模型参数，这里可以指定最大迭代次数，默认值为 100。

我们使用 fit 函数，传入样本特征矩阵，模型会自动生成聚类器，然后使用 prediction=gmm.predict(data) 来对数据进行聚类，传入你想进行聚类的数据，可以得到聚类结果 prediction。

## 王者荣耀数据聚类实战

[代码](.\learnDS\hero_em.py)

# 课时30

## Apriori概念

Apriori算法主要解决关联规则挖掘的问题，可以让我们你在数据集中发现项之间的关系，比如“购物篮分析”就是一个常见的场景，这个场景可以从消费者交易记录中发掘商品与商品之间的关联关系，进而通过商品捆绑销售或者相关推荐的方式带来更多的销售量。

### 支持度

支持度是个百分比，它指的是某个商品组合出现的次数与总次数之间的比例。支持度越高，代表这个组合出现的频率越大。

### 置信度

它指的就是当你购买了商品 A，会有多大的概率购买商品 B。这是个条件概念，就是说在 A 发生的情况下，B 发生的概率是多少。

### 提升度

商品推荐时的考虑重点，描述的是“商品 A 的出现，对商品 B 的出现概率提升的”程度。

> 提升度 (A→B)= 置信度 (A→B)/ 支持度 (B)
>
> 1. 提升度 (A→B)>1：代表有提升；
> 2. 提升度 (A→B)=1：代表有没有提升，也没有下降；
> 3. 提升度 (A→B)<1：代表有下降。

## Apriori的工作原理

Apriori 算法其实就是查找频繁项集 (frequent itemset) 的过程，所以首先我们需要定义什么是频繁项集。我们认为支持度大于等于**最小支持度 (Min Support) 阈值**的项集，所以小于最小值支持度的项目就是非频繁项集，而大于等于最小支持度的项集就是频繁项集，假设我随机指定最小支持度是 50%。

### 原始流程

1. K=1，计算 K 项集的支持度，K指的是一个项集中元素的个数，当K＞1时需要对所有元素做排列组合；
2. 筛选掉小于最小支持度阈值的项集；
3. 如果项集为空，则对应 K-1 项集的结果为最终结果。否则 K=K+1，重复 1-3 步。

我们可以看到原始流程的缺点：

1. 可能产生大量的候选集。因为采用排列组合的方式，把可能的项集都组合出来了；
2. 每次计算都需要重新扫描数据集，来计算每个项集的支持度。

### 改进流程：FP-Growth 算法

#### Step1：创建项头表

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230205104520760.png" alt="image-20230205104520760" style="zoom: 67%;" />

扫描一遍数据集，对于满足最小支持度的单个项（K=1 项集）按照支持度从高到低进行排序，这个过程中删除了不满足最小支持度的项。

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230205104623475.png" alt="image-20230205104623475" style="zoom:67%;" />

#### Step2：构造FP树

再扫描一次数据集，对于每一条数据，按照支持度从高到低的顺序进行创建节点（也就是第一步中项头表中的排序结果），节点如果存在就将计数 count+1，如果不存在就进行创建。同时在创建的过程中，需要更新项头表的链表。

![image-20230205110240080](C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230205110240080.png)

#### Step3：挖掘频繁项集

计算每个项的“条件模式基”，它指的是以要挖掘的节点为叶子节点，自底向上求出 FP 子树，然后将 FP 子树的祖先节点设置为叶子节点之和。

如下图，计算“啤酒”节点的条件模式基，由于1、5号订单不包含啤酒，所以这条路径上的三件商品各减去2，祖先节点的支持度小于阈值被剪枝，因此啤酒的条件模式基为空。

![image-20230205103729732](C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230205103729732.png)

同理计算其他项得到频繁项集。

#### 优势

1. 创建了一棵 FP 树来存储频繁项集。在创建前对不满足最小支持度的项进行删除，减少了存储空间。我稍后会讲解如何构造一棵 FP 树；
2. 整个生成过程只遍历数据集 2 次，大大减少了计算量。

# 课时31

[代码](.\learnDS\directorAssociation.py)
