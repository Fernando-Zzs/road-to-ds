# 课时17

## 决策树工作原理

决策树是把我们以前的经验分类总结的模型。在做决策树的时候，会经历两个阶段：构造和剪枝。

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230201223209008.png" alt="image-20230201223209008" style="zoom:67%;" />

### 构造

构造时需要考虑三种节点（一个节点代表一种属性）：

- 根节点
- 内部节点
- 叶节点

需要解决三个重要问题：

- 选择哪个属性作为根节点
- 选择哪些属性作为子节点
- 什么时候停止并得到目标状态，即叶节点

### 剪枝

剪枝就是给决策树瘦身，避免出现**过拟合**现象。造成过拟合的原因之一就是因为训练集中**样本量较小**。如果决策树选择的属性过多，构造出来的决策树一定能够“完美”地把训练集中的样本分类，但是这样就会把训练集中一些数据的特点当成所有数据的特点，但这个特点不一定是全部数据的特点，这就使得这个决策树在真实的数据分类中出现错误，也就是模型的“泛化能力”差。

#### 预剪枝

在决策树构造时就进行剪枝。方法是在**构造的过程中**对节点进行评估，如果对某个节点进行划分，在验证集中不能带来准确性的提升，那么对这个节点进行划分就没有意义，这时就会把当前节点作为叶节点，不对其进行划分。

#### 后剪枝

在**生成决策树之后**再进行剪枝，通常会从决策树的叶节点开始，**逐层向上**对每个节点进行评估。如果剪掉这个节点子树，与保留该节点子树在分类准确性上差别不大，或者剪掉该节点子树，能在验证集中带来准确性的提升，那么就可以把该节点子树进行剪枝。

方法是：用这个节点子树的叶子节点来替代该节点，类标记为这个节点子树中最频繁的那个类。

## 如何选择决策树的节点

### 纯度

决策树构造过程就是寻找纯净划分的过程。一个集合中分歧最小的纯度即为最高。纯度也就是让目标变量的分歧最小。

### 信息熵（Entropy）

表示信息的不确定度，在信息论中，随机离散事件出现的概率存在着不确定性。为了衡量这种信息的不确定性，信息学之父香农引入了信息熵的概念，并给出了计算信息熵的数学公式：

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230201224102632.png" alt="image-20230201224102632" style="zoom:50%;" />

p(i|t) 代表了节点 t 为分类 i 的概率。

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230201224647325.png" alt="image-20230201224647325" style="zoom:50%;" />

它能帮我们反映出来这个信息的不确定度。当不确定性越大时，它所包含的信息量也就越大，信息熵也就越高。

信息熵越大，纯度越低。当集合中所有样本均匀混合时，信息熵最大，纯度最低。

### 不纯度的指标

我们在构造决策树的时候，会基于纯度来构建。而经典的 “不纯度”的指标有三种，分别是信息增益（ID3 算法）、信息增益率（C4.5 算法）以及基尼指数（Cart 算法）。

#### ID3算法

##### 计算方法

计算的是信息增益，指的是划分可以带来纯度的提高，信息熵的下降。

它的计算公式，是父亲节点的信息熵减去所有子节点的信息熵。在计算的过程中，我们会计算每个子节点的**归一化信息熵**，即按照每个子节点在父节点中出现的概率，来计算这些子节点的信息熵。所以信息增益的公式可以表示为：

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230201224600016.png" alt="image-20230201224600016" style="zoom:50%;" />

公式中 D 是父亲节点，Di 是子节点，Gain(D,a) 中的 a 作为 D 节点的属性选择。

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230201224701944.png" alt="image-20230201224701944" style="zoom:50%;" />

所有的属性都计算一次信息增益，选择信息增益值最大的作为这一层的节点。

##### 优缺点分析

优点：ID3 的算法规则相对简单，可解释性强。

缺点：

- ID3 算法倾向于选择取值比较多的属性。有些属性可能对分类任务没有太大作用，但是他们仍然可能会被选为最优属性。
- 噪声敏感。训练数据如果有少量错误，可能会产生决策树分类错误。

#### C4.5算法

##### 计算方法

相比较ID3算法，此算法采用了信息增益率：

> 信息增益率=信息增益 / 属性熵

当属性有很多值的时候，相当于被划分成了许多份，虽然信息增益变大了，但是对于 C4.5来说，属性熵也会变大，所以整体的信息增益率并不大。

##### 优缺点分析

优点：

- 用信息增益率代替了信息增益，解决了噪声敏感的问题

- 采用悲观剪枝

  ID3 构造决策树的时候，容易产生过拟合的情况。在 C4.5 中，会在决策树构造之后采用悲观剪枝（PEP），这样可以提升决策树的泛化能力。
  悲观剪枝是后剪枝技术中的一种，通过递归估算每个内部节点的分类错误率，比较剪枝前后这个节点的分类错误率来决定是否对其进行剪枝。这种剪枝方法不再需要一个单独的测试数据集。

- 离散化处理连续属性

  C4.5 可以处理连续属性的情况，对连续的属性进行离散化的处理。比如打篮球存在的“湿度”属性，不按照“高、中”划分，而是按照湿度值进行计算，那么湿度取什么值都有可能。该怎么选择这个阈值呢，C4.5 选择具有最高信息增益的划分所对应的阈值。

- 处理缺失值

  针对数据集不完整的情况，C4.5 也可以进行处理。当在计算某个属性的信息增益率时，如果某个样本的属性值缺失，可以先跳过，计算后的值再和`有效样本数/总样本数`相乘

缺点：需要对数据集进行多次扫描，算法效率相对较低。

# 课时18

## CART决策树

ID3和C4.5决策树可以生成二叉树或多叉树，CART只支持二叉树。CART决策树既可以作分类树，又可以作回归树。

分类树可以处理**离散**数据，也就是数据种类有限的数据，它输出的是样本的类别，而回归树可以对**连续**型的数值进行预测，也就是数据在某个区间内都有取值的可能，它输出的是一个数值。

### 分类树的工作原理

决策树的核心是寻找纯净的划分，在分类问题上，CART与C4.5类似，不同地方在于采用的指标是基尼系数。

基尼系数本身反应了样本的不确定度。当基尼系数越小的时候，说明样本之间的差异性小，不确定程度低。分类的过程本身是一个不确定度降低的过程，即纯度的提升过程。所以CART 算法在构造分类树的时候，会选择基尼系数最小的属性作为属性的划分。

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202113838345.png" alt="image-20230202113838345" style="zoom:50%;" />

这里 p(Ck|t) 表示节点 t 属于类别 Ck 的概率，节点 t 的基尼系数为 1 减去各类别 Ck 概率平方和。

在 CART 算法中，基于基尼系数对特征属性进行二元分裂，假设属性 A 将节点 D 划分成了D1 和 D2，如下图所示：

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202113950898.png" alt="image-20230202113950898" style="zoom:50%;" />

节点 D 的基尼系数等于子节点 D1 和 D2 的归一化基尼系数之和，用公式表示为：

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202114011521.png" alt="image-20230202114011521" style="zoom:50%;" />

归一化基尼系数代表的是每个子节点的基尼系数乘以该节点占整体父亲节点 D 中的比例。

```python
# 创建 CART 分类树
clf = DecisionTreeClassifier(criterion='gini')
# 拟合构造 CART 分类树
clf = clf.fit(train_features, train_labels)
# 用 CART 分类树做预测
test_predict = clf.predict(test_features)
# 预测结果与测试集结果作比对
score = accuracy_score(test_labels, test_predict)
print("CART 分类树准确率 %.4lf" % score)
```



### 回归树的工作原理

CART 回归树划分数据集的过程和分类树的过程是一样的，只是回归树得到的预测结果是连续值，而且评判“不纯度”的指标不同。回归树使用的是样本的离散程度。

- 最小绝对偏差（LAD）

  <img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202123422951.png" alt="image-20230202123422951" style="zoom:25%;" />

- 最小二乘偏差（LSD）

  <img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202123459293.png" alt="image-20230202123459293" style="zoom: 50%;" />

```python
# 创建 CART 回归树
dtr=DecisionTreeRegressor()
# 拟合构造 CART 回归树
dtr.fit(train_features, train_price)
# 预测测试集中的房价
predict_price = dtr.predict(test_features)
# 测试集的结果评价
print('回归树二乘偏差均值:', mean_squared_error(test_price, predict_price))
print('回归树绝对值偏差均值:', mean_absolute_error(test_price, predict_price)) 
```

### CART决策树的剪枝

#### Step1

采用cost-complexity prune（CPP，代价复杂度）方法，这是一种后剪枝方法，使用的指标是：**表面误差率增益值**。表示的是：节点 t 的子树被剪枝后的误差变化除以剪掉的叶子数量。

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202123823337.png" alt="image-20230202123823337" style="zoom:33%;" />

其中， C(t) 表示节点 t 的子树被剪枝后节点 t 的误差，Tt 代表以 t 为根节点的子树；C(Tt) 表示节点 t 的子树没被裁剪时子树 Tt 的误差；|Tt|代表子树 Tt 的叶子数，剪枝后，T 的叶子数减少了|Tt|-1。

我们希望剪枝前后误差最小，所以我们要寻找的就是最小α值对应的节点，把它剪掉。这时候生成了第一个子树。（重复上面的过程，继续剪枝，直到最后只剩下根节点，即为最后一个子树。）

#### Step2

得到了剪枝后的子树集合后，我们需要用验证集对所有子树的误差计算一遍。可以通过计算每个子树的基尼指数或者平方误差，取误差最小的那个树，得到我们想要的结果。

# 课时19

## 泰坦尼克号乘客生存预测

### 关键流程

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202135644382.png" alt="image-20230202135644382" style="zoom:67%;" />

### 数据探索

如何进行数据探索呢？这里有一些函数你需要了解：

- 使用 info() 了解数据表的基本情况：行数、列数、每列的数据类型、数据完整度；
- 使用 describe() 了解数据表的统计情况：总数、平均值、标准差、最小值、最大值等；
- 使用 describe(include=[‘O’]) 查看字符串类型（非数字）的整体情况；
- 使用 head 查看前几行数据（默认是前 5 行）；
- 使用 tail 查看后几行数据（默认是最后 5 行）。

### 清洗数据

对于缺失的数据，考虑使用平均值、最高频值的方式补全，当缺失值过多时放弃补全。

其他情况参照前面所讲的准则进行数据清洗。

### 特征选择

特征选择是分类器的关键。特征选择不同，得到的分类器也不同。那么我们该选择哪些特征做生存的预测呢？这是基于常识和经验的工作，判断哪些列可能会与预测分类有关，提取出来作为训练的特征。

当特征值不是数值类型时，可以使用DictVectorizer类进行转化，这个过程会改变特征值的数量。比如Sex列会拆分成“Sex=female”和“Sex=male”两列，结果用0/1表示。

```python
from sklearn.feature_extraction import DictVectorizer
dvec=DictVectorizer(sparse=False)
train_features=dvec.fit_transform(train_features.to_dict(orient='record'))
```

这里用到fit_transform，是fit和transform的组合，既包括了训练又包含了转换。

transform()和fit_transform()二者的功能都是对数据进行某种统一处理（比如标准化~N(0,1)，将数据缩放(映射)到某个固定区间，归一化，正则化等）

不同点在于fit_transform(trainData)对部分数据先拟合fit，找到该part的整体指标，如均值、方差、最大值最小值等等（根据具体转换的目的），然后对该trainData进行转换transform，从而实现数据的标准化、归一化等等。

### 决策树模型

目前为止，sklearn 中只实现了 ID3 与 CART 决策树，所以我们暂时只能使用这两种决策树，在构造DecisionTreeClassifier 类时，其中有一个参数是 criterion，意为标准。它决定了构造的分类树是采用 ID3 分类树，还是 CART 分类树，对应的取值分别是 entropy或者 gini。

除了设置 criterion 采用不同的决策树算法外，一般建议使用默认的参数，默认参数不会限制决策树的最大深度，不限制叶子节点数，认为所有分类的权重都相等等。当然你也可以调整这些参数，来创建不同的决策树模型。

在构造决策树分类器后，我们可以使用 fit 方法让分类器进行拟合。

### 模型评估与预测

使用 predict 方法对新数据进行预测，得到预测的分类结果，也可以使用 score 方法得到分类器的准确率。模型准确率需要考虑是否有测试集的实际结果可以做对比，当测试集没有真实结果可以对比时，需要使用 **K 折交叉验证cross_val_score **的方法。

#### K折交叉验证

使用 K 折交叉验证的方式，交叉验证是一种常用的验证分类准确率的方法，原理是拿出大部分样本进行训练，少量的用于分类器的验证。K 折交叉验证，就是做 K 次交叉验证，每次选取 K 分之一的数据作为验证，其余作为训练。轮流 K 次，取平均值。K 折交叉验证的原理是这样的：
1. 将数据集平均分割成 K 个等份；
2. 使用 1 份数据作为测试数据，其余作为训练数据；
3. 计算测试准确率；
4. 使用不同的测试集，重复 2、3 步骤。

### 决策树可视化

使用 Graphviz 可视化工具帮我们把决策树呈现出来。 

# 课时20

## 贝叶斯原理

### 逆向概率

所谓“逆向概率”是相对“正向概率”而言。正向概率即了解事情全貌，以上帝视角来判断一件事发生的概率是多少；逆向概率则是建立在主观判断的基础上，在我们不了解所有客观事实的情况下，先估计一个值，然后根据实际结果不断进行修正。

### 先验概率

通过经验来判断事情发生的概率，就是先验概率。

### 后验概率

发生结果之后推测原因的概率，属于条件概率的一种。

### 条件概率

事件A在另一个事件B已经发生的条件下发生的概率，表示为 P(A|B)，读作“在 B 发生的条件下 A 发生的概率”。

### 似然函数

你可以把概率模型的训练过程理解为求参数估计的过程。举个例子，如果一个硬币在 10 次抛落中正面均朝上。那么你肯定在想，这个硬币是均匀的可能性是多少？这里硬币均匀就是个参数，似然函数就是用来衡量这个模型的参数。似然在这里就是可能性的意思，它是关于统计参数的函数。

### 贝叶斯原理

贝叶斯原理就是求解后验概率，通常我们知道先验概率P(Bi)，知道观测值在不同类别发生情况下的条件概率P(Bi|A)，就可以求出观测值的情况下不同类别发生的后验概率：

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202204028518.png" alt="image-20230202204028518" style="zoom:50%;" />

## 朴素贝叶斯

简单、强大的预测建模算法，它假设每个输入变量是独立的。这是一个强硬的假设，实际情况并不一定，但是这项技术对于绝大部分的复杂问题仍然非常有效。

朴素贝叶斯模型由两种类型的概率组成：
1. 每个**类别的概率**P(Cj)
2. 每个属性的**条件概率**P(Ai|Cj)

为了训练朴素贝叶斯模型，我们需要先给出训练数据，以及这些数据对应的分类。那么上面这两个概率，也就是类别概率和条件概率。他们都可以从给出的训练数据中计算出来。一旦计算出来，概率模型就可以使用贝叶斯原理对新数据进行预测。

## 概念区别

贝叶斯原理、贝叶斯分类和朴素贝叶斯这三者之间是有区别的。

贝叶斯原理是最大的概念，它解决了概率论中“逆向概率”的问题，在这个理论基础上，人们设计出了贝叶斯分类器，朴素贝叶斯分类是贝叶斯分类器中的一种，也是最简单，最常用的分类器。朴素贝叶斯之所以朴素是因为它假设属性是相互独立的，因此对实际情况有所约束，如果属性之间存在关联，分类准确率会降低。不过好在对于大部分情况下，朴素贝叶斯的分类效果都不错。

## 朴素贝叶斯分类的工作原理

朴素贝叶斯分类是常用的贝叶斯分类方法。我们看到一个陌生人，要做的第一件事情就是判断 TA 的性别，判断性别的过程就是一个分类的过程。根据以往的经验，我们通常会从身高、体重、鞋码、头发长短、服饰、声音等角度进行判断。这里的“经验”就是一个训练好的关于性别判断的模型，其训练数据是日常中遇到的各式各样的人，以及这些人实际的性别数据。

### 离散数据案例

#### 问题描述

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202205549457.png" alt="image-20230202205549457" style="zoom:50%;" />

给定上面的数据，现在有一个新的数据：身
高“高”、体重“中”，鞋码“中”，请问这个人是男还是女？

#### 推导过程

##### Step1

属性：3个，分别用A1, A2, A3 代表身高 = 高、体重 = 中、鞋码 = 中

类别：2个，C1,C2 分别是：男、女

##### Step2

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202205756497.png" alt="image-20230202205756497" style="zoom: 33%;" />

因为一共有 2 个类别，所以我们只需要求得 P(C1|A1A2A3) 和 P(C2|A1A2A3) 的概率即可，然后比较下哪个分类的可能性大，就是哪个分类结果。

因为 P(A1A2A3) 都是固定的，我们想要寻找使得 P(Cj|A1A2A3) 的最大值，就等价于求 P(A1A2A3|Cj)P(Cj) 最大值。

##### Step3

我们假定 Ai 之间是相互独立的，那么：
P(A1A2A3|Cj)=P(A1|Cj)P(A2|Cj)P(A3|Cj)

然后我们需要从 Ai 和 Cj 中计算出 P(Ai|Cj) 的概率，带入到上面的公式得出P(A1A2A3|Cj)，最后找到使得 P(A1A2A3|Cj) 最大的类别 Cj。

### 连续数据案例

#### 问题描述

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202210232698.png" alt="image-20230202210232698" style="zoom:50%;" />

如果给你一个新的数据，身高 180、体重 120，鞋码 41，请问该人是男是女呢？

#### 推导过程

##### Step1

将属性假设为正态分布，通过样本计算出均值和方差，也就是得到正态分布的密度函数。有了密度函数，就可以把值代入，算出某一点的密度函数的值。这些计算可以通过 EXCEL 的NORMDIST(x,mean,standard_dev,cumulative) 函数实现，一共有 4 个参数：
1. x：正态分布中，需要计算的数值；
2. Mean：正态分布的平均值；
3. Standard_dev：正态分布的标准差；
4. Cumulative：取值为逻辑值，即 False 或 True。它决定了函数的形式。当为 TRUE
时，函数结果为累积分布；为 False 时，函数结果为概率密度。

##### Step2

与离散案例一样，分别计算各个类别下的P(A1A2A3|Ci)=P(A1|Ci)P(A2|Ci)P(A3|Ci)，取最大值即为分类结果。

## 朴素贝叶斯的工作流程

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202210701922.png" alt="image-20230202210701922" style="zoom:67%;" />

第一阶段：人工完成的阶段，对每个特征属性进行适当划分，对一部分数据进行分类，形成训练样本；

第二阶段：生成分类器，计算每个类别在训练样本中的出现频率、每个特征属性划分在每个类别情况下的条件概率，输出分类器。

第三阶段：输入新数据，得到预测的分类结果。

# 课时21

## 朴素贝叶斯的应用

朴素贝叶斯分类最适合的场景就是文本分类、情感分析和垃圾邮件识别。其中情感分析和垃圾邮件识别都是通过文本来进行判断。从这里你能看出来，这三个场景本质上都是文本分类，这也是朴素贝叶斯最擅长的地方。所以朴素贝叶斯也常用于自然语言处理 NLP 的工具。

## sklearn机器学习包

sklearn 的全称叫 Scikit-learn，它给我们提供了 3 个朴素贝叶斯分类算法，分别是高斯朴素贝叶斯（GaussianNB）、多项式朴素贝叶斯（MultinomialNB）和伯努利朴素贝叶斯（BernoulliNB）。

这三种算法适合应用在不同的场景下，我们应该根据特征变量的不同选择不同的算法：

- 高斯朴素贝叶斯：特征变量是连续变量，符合高斯分布，比如说人的身高，物体的长度。
- 多项式朴素贝叶斯：特征变量是离散变量，符合多项分布，在文档分类中特征变量体现在一个单词出现的次数，或者是单词的 TF-IDF 值等。
- 伯努利朴素贝叶斯：特征变量是布尔变量，符合 0/1 分布，在文档分类中特征是单词是否出现。

## TF-IDF值

TF-IDF 是一个统计方法，用来评估某个词语对于一个文件集或文档库中的其中一份文件的重要程度。实际上是词频和逆向文档频率两者的乘积，倾向于找到 TF 和IDF 取值都高的单词作为区分，即这个单词在一个文档中出现的次数多，同时又很少出现在其他文档中。这样的单词适合用于分类。

### TF

词频（Term Frequency）计算了一个单词在文档中出现的次数，它认为一个单词的重要性和它在文档中出现的次数呈正比。

### IDF

逆向文档频率（Inverse Document Frequency），是指一个单词在文档中的区分度。它认为一个单词出现在的文档数越少，就越能通过这个单词把该文档和其他文档区分开。**IDF 越大就代表该单词的区分度越大。**

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202211710598.png" alt="image-20230202211710598" style="zoom:50%;" />

为什么 IDF 的分母中，单词出现的文档数要加 1 呢？因为有些单词可能不会存在文档中，为了避免分母为 0，统一给单词出现的文档数都加 1。

## 如何计算TF-IDF值

通过TfidfVectorizer 类

```python
from sklearn.feature_extraction.text import TfidfVectorizer

# TfidfVectorizer 类的构造函数
# TfidfVectorizer(stop_words=stop_words, token_pattern=token_pattern)

tfidf_vec = TfidfVectorizer()
documents = [
 'this is the bayes document',
 'this is the second second document',
 'and the third one',
 'is this the document'
]
tfidf_matrix = tfidf_vec.fit_transform(documents) # 进行模型的拟合，可以返回每个单词在每个文档中的TF-IDF值

print('不重复的词:', tfidf_vec.get_feature_names())
print('每个单词的 ID:', tfidf_vec.vocabulary_)
print('每个单词的 tfidf 值:', tfidf_matrix.toarray())
```

- stop_words表示自定义停用词，输入一个List，包含在分类中没有用的词（TF高IDF很低），这么做可以节省空间和计算时间。
- token_pattern表示过滤规则，输入正则表达式

## 如何对文档进行分类

<img src="C:\Users\Lenvov\AppData\Roaming\Typora\typora-user-images\image-20230202213100549.png" alt="image-20230202213100549" style="zoom:67%;" />

### 文档分词

英文文档可以使用NTLK包进行分词，包含了停用词、分词和标注方法等功能；在中文文档中，最常用的是 jieba 包。jieba 包中包含了中文的停用词 stop words 和分词方法。

```python
import nltk
word_list = nltk.word_tokenize(text) # 分词
nltk.pos_tag(word_list) # 标注单词的词性

import jieba
word_list = jieba.cut(text) # 中文分词
```

### 加载停用词表

我们需要自己读取停用词表文件，从网上可以找到中文常用的停用词保存在stop_words.txt，然后利用 Python 的文件读取函数读取文件，保存在 stop_words 数组中。

```python
stop_words = [line.strip().decode('utf-8') for line in io.open('stop_words.txt').readline()]
```

### 计算单词的权重

也就是计算TF-IDF特征空间features，选出来的分词就是特征。

```python
tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5) # max_df参数用来描述单词在文档中的最高出现率阈值，如果超过这个值则不作为分词统计
features = tf.fit_transform(train_contents)
```

### 生成朴素贝叶斯分类器

```python
# 多项式贝叶斯分类器
from sklearn.naive_bayes import MultinomialNB 
clf = MultinomialNB(alpha=0.001).fit(train_features, train_labels)
```

我们将特征训练集的特征空间 train_features，以及训练集对应的分类 train_labels 传递给贝叶斯分类器 clf，它会自动生成一个符合特征空间和对应分类的分类器。

这里我们采用的是多项式贝叶斯分类器，其中 alpha 为平滑参数。为什么要使用平滑呢？因为如果一个单词在训练样本中没有出现，这个单词的概率就会被计算为 0。但训练集样本只是整体的抽样情况，我们不能因为一个事件没有观察到，就认为整个事件的概率为 0。为了解决这个问题，我们需要做平滑处理。

- 当 alpha=1 时，使用的是 Laplace 平滑。Laplace 平滑就是采用加 1 的方式，来统计没有出现过的单词的概率。这样当训练样本很大的时候，加 1 得到的概率变化可以忽略不计，也同时避免了零概率的问题。
- 当 0<alpha<1 时，使用的是 Lidstone 平滑。对于 Lidstone 平滑来说，alpha 越小，迭代次数越多，精度越高。我们可以设置 alpha 为 0.001。

### 用分类器做预测

```python
# 得到测试集的特征矩阵
test_tf = TfidfVectorizer(stop_words=stop_words, max_df=0.5, vocabulary=train_vocabulary)
# 得到测试集的特征矩阵 test_features
test_features=test_tf.fit_transform(test_contents)
# 传入测试集的特征矩阵，得到分类结果
predicted_labels=clf.predict(test_features)
```

### 计算准确率

在模型评估中，sklearn 提供了 metrics 包，帮我们对预测结果与实际结果进行评估。

```python
from sklearn import metrics
print metrics.accuracy_score(test_labels, predicted_labels)
```

