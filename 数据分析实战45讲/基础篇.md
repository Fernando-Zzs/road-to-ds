# 课时1

## 数据分析的全景图

- 数据采集
  - 数据源
    - 爬虫
    - 日志采集
    - 传感器
  - 工具使用
    - 八爪鱼
    - 火车采集器
    - 搜集客
  - 爬虫编写
    - phantomjs
    - Scarp
    - lxml
    - Selenium
- 数据挖掘
  - 数学基础
    - 概率论与数据统计
    - 线性代数
    - 图论
    - 最优化方法
  - 基本流程
    - 商业理解
    - 数据理解
    - 数据准备
    - 模型建立
    - 模型评估
    - 上线发布
  - 十大算法
    - 分类算法
      - C45
      - 朴素贝叶斯
      - SVM
      - KNN
      - Adaboost
      - CART
    - 聚类算法
      - K-Means
      - EM
    - 关联分析
      - Apriori
    - 连接分析
      - PageRank
  - 实战
    - 如何识别手写字
    - 如何进行乳腺癌症检测
    - 如何对文档进行归类
    - ...
- 数据可视化
  - Python 数据清洗 挖掘
    - matplotlib
    - Seaborn
  - 第三方工具
    - 微图
    - DataV
    - Data GIF Maker

## 牢记原则

1. 不重复造轮子
2. 工具决定效率
3. 熟练度

# 课时2

## 数据挖掘的基本流程

数据挖掘的过程可以分成以下 6 个步骤。

1. 商业理解：数据挖掘不是我们的目的，我们的目的是更好地帮助业务，所以第一步我们
    要从商业的角度理解项目需求，在这个基础上，再对数据挖掘的目标进行定义。
2. 数据理解：尝试收集部分数据，然后对数据进行探索，包括数据描述、数据质量验证
    等。这有助于你对收集的数据有个初步的认知。
3. 数据准备：开始收集数据，并对数据进行清洗、数据集成等操作，完成数据挖掘前的准
    备工作。
4. 模型建立：选择和应用各种数据挖掘模型，并进行优化，以便得到更好的分类结果。
5. 模型评估：对模型进行评价，并检查构建模型的每个步骤，确认模型是否实现了预定的
    商业目标。
6. 上线发布：模型的作用是从数据中找到金矿，也就是我们所说的“知识”，获得的知识
    需要转化成用户可以使用的方式，呈现的形式可以是一份报告，也可以是实现一个比较
    复杂的、可重复的数据挖掘过程。数据挖掘结果如果是日常运营的一部分，那么后续的
    监控和维护就会变得重要。

## 数据挖掘的十大算法

- 分类算法：C4.5，朴素贝叶斯（Naive Bayes），SVM，KNN，Adaboost，CART
-  聚类算法：K-Means，EM
- 关联分析：Apriori
- 连接分析：PageRank

### C4.5

C4.5 是决策树的算法，它创造性地在决策树构造过程中就进行了剪枝，并且可以处理连续的属性，也能对不完整的数据进行处理。它可以说是决策树分类中，具有里程碑式意义的算法。

### 朴素贝叶斯（Naive Bayes）

朴素贝叶斯模型是基于概率论的原理，它的思想是这样的：对于给出的未知物体想要进行分类，就需要求解在这个未知物体出现的条件下各个类别出现的概率，哪个最大，就认为这个未知物体属于哪个分类。

### SVM

SVM 的中文叫支持向量机，英文是 Support Vector Machine，简称 SVM。SVM 在训练中建立了一个超平面的分类模型。

### KNN

KNN 也叫 K 最近邻算法，英文是 K-Nearest Neighbor。所谓 K 近邻，就是每个样本都可以用它最接近的 K 个邻居来代表。如果一个样本，它的 K 个最接近的邻居都属于分类 A，那么这个样本也属于分类 A。

### AdaBoost

Adaboost 在训练中建立了一个联合的分类模型。boost 在英文中代表提升的意思，所以Adaboost 是个构建分类器的提升算法。它可以让我们多个弱的分类器组成一个强的分类器，所以 Adaboost 也是一个常用的分类算法。

### CART

CART 代表分类和回归树，英文是 Classification and Regression Trees。像英文一样，它构建了两棵树：一棵是分类树，另一个是回归树。和 C4.5 一样，它是一个决策树学习方法。

### Apriori

Apriori 是一种挖掘关联规则（association rules）的算法，它通过挖掘频繁项集（frequent item sets）来揭示物品之间的关联关系，被广泛应用到商业挖掘和网络安全等领域中。频繁项集是指经常出现在一起的物品的集合，关联规则暗示着两种物品之间可能存在很强的关系。

### K-Means

K-Means 算法是一个聚类算法。你可以这么理解，最终我想把物体划分成 K 类。假设每个类别里面，都有个“中心点”，即意见领袖，它是这个类别的核心。现在我有一个新点要归类，这时候就只要计算这个新点与 K 个中心点的距离，距离哪个中心点近，就变成了哪个类别。

### EM

EM 算法也叫最大期望算法，是求参数的最大似然估计的一种方法。原理是这样的：假设我们想要评估参数 A 和参数 B，在开始状态下二者都是未知的，并且知道了 A 的信息就可以得到 B 的信息，反过来知道了 B 也就得到了 A。可以考虑首先赋予 A 某个初值，以此得到B 的估值，然后从 B 的估值出发，重新估计 A 的取值，这个过程一直持续到收敛为止。
EM 算法经常用于聚类和机器学习领域中。

### PageRank

PageRank 起源于论文影响力的计算方式，如果一篇文论被引入的次数越多，就代表这篇论文的影响力越强。同样PageRank 被 Google 创造性地应用到了网页权重的计算中：当一个页面链出的页面越多，说明这个页面的“参考文献”越多，当这个页面被链入的频率越高，说明这个页面被引用的次数越高。基于这个原理，我们可以得到网站的权重划分。

## 数据挖掘的数学原理

### 概率论与数理统计

在数据挖掘里使用到概率论的地方就比较多了。比如条件概率、独立性的概念，以及随机变量、多维随机变量的概念。很多算法的本质都与概率论相关，所以说概率论与数理统计是数据挖掘的重要数学基础。

### 线性代数

向量和矩阵是线性代数中的重要知识点，它被广泛应用到数据挖掘中，比如我们经常会把对象抽象为矩阵的表示，一幅图像就可以抽象出来是一个矩阵，我们也经常计算特征值和特征向量，用特征向量来近似代表物体的特征。这个是大数据降维的基本思路。基于矩阵的各种运算，以及基于矩阵的理论成熟，可以帮我们解决很多实际问题，比如PCA 方法、SVD 方法，以及 MF、NMF 方法等在数据挖掘中都有广泛的应用。

### 图论

社交网络的兴起，让图论的应用也越来越广。人与人的关系，可以用图论上的两个节点来进行连接，节点的度可以理解为一个人的朋友数。我们都听说过人脉的六度理论，在Facebook 上被证明平均一个人与另一个人的连接，只需要 3.57 个人。当然图论对于网络结构的分析非常有效，同时图论也在关系挖掘和图像分割中有重要的作用。

### 最优化方法

最优化方法相当于机器学习中自我学习的过程，当机器知道了目标，训练后与结果存在偏差就需要迭代调整，那么最优化就是这个调整的过程。一般来说，这个学习和迭代的过程是漫长、随机的。最优化方法的提出就是用更短的时间得到收敛，取得更好的效果。

# 课时3

## 基本数据结构

### 列表：[]

列表是 Python 中常用的数据结构，相当于数组，具有增删改查的功能，我们可以使用len() 函数获得 lists 中元素的个数；使用 append() 在尾部添加元素，使用 insert() 在列表中插入元素，使用 pop() 删除尾部的元素。

### 元组：()

元组 tuple 和 list 非常类似，但是 tuple 一旦初始化就不能修改。因为不能修改所以没有append(), insert() 这样的方法，可以像访问数组一样进行访问，比如 tuples[0]，但不能赋值。

### 字典 : {}

字典其实就是{key, value}，多次对同一个 key 放入 value，后面的值会把前面的值冲掉，同样字典也有增删改查。增加字典的元素相当于赋值，比如 score[‘zhaoyun’] = 98，删除一个元素使用 pop，查询使用 get，如果查询的值不存在，我们也可以给一个默认值，比如 score.get(‘yase’,99)。

### 集合：set([])

集合 set 和字典 dictory 类似，不过它只是 key 的集合，不存储 value。同样可以增删查，增加使用 add，删除使用 remove，查询看某个元素是否在这个集合里，使用 in。

# 课时4

## NumPy数组的优势

- 列表 list 的元素在系统内存中是分散存储的，而 NumPy 数组存储在一个均匀连续的内存块中，数组计算遍历所有的元素，不像列表 list 还需要对内存地址进行查找，从而节省了计算资源。
- 在内存访问模式中，缓存会直接把字节块从 RAM 加载到 CPU 寄存器中。因为数据连续的存储在内存中，NumPy 直接利用现代 CPU 的矢量化指令计算，加载寄存器中的多个连续浮点数。
- NumPy 中的矩阵计算可以采用多线程的方式，充分利用多核 CPU 计算资源，大大提升了计算效率。
- 避免采用隐式拷贝，而是采用就地操作的方式。举个例子，如果我想让一个数值 x 是原来的两倍，可以直接写成 x\*=2，而不要写成 y=x\*2。

## NumPy的对象

### ndarray（N-dimensional array object）

NumPy 数组中，维数称为秩（rank），每一个线性的数组称为一个轴（axes），其实秩就是描述轴的数量。

```python
b = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])
```

### ufunc（universal function object）

它能对数组中每个元素进行函数操作。NumPy 中很多 ufunc 函数计算速度非常快，因为都是采用 C 语言实现的。因为在对多维进行计算的时候需要进行循环，c语言的循环比python的循环效率要高。

包括三角函数、四则运算、比较运算、布尔运算、自定义构建

## 结构数组dtype

在 C 语言里，可以定义结构数组，也就是通过 struct 定义结构类型，结构中的字段占据连续的内存空间，每个结构体占用的内存大小都相同，在NumPy中定义的方式如下：

```python
persontype = np.dtype({
 'names':['name', 'age', 'chinese', 'math', 'english'],
 'formats':['S32','i', 'i', 'i', 'f']})
peoples = np.array([("ZhangFei",32,75,100, 90),("GuanYu",24,85,96,88.5),
 ("ZhaoYun",28,85,92,96.5),("HuangZhong",29,65,85,100)],
dtype=persontype)
```

numpy 中的字符编码来表示数据类型的定义：

<img src=".\images\image-20230131172021851.png" alt="image-20230131172021851" style="zoom:50%;" />

## 连续数组的创建

```python
x1 = np.arange(1,11,2) # 初始值 终止值（闭区间） 步长
x2 = np.linspace(1,9,5) # 初始值 终止值（开区间） 元素个数 
```

## 统计函数

对于一个ndarray，想要知道这些数据中的最大值、最小值、平均值，是否符合正态分布，方差、标准差多少等等信息。统计函数可以让你更清楚地对这组数据有认知。

- amin
- amax
- ptp
- percentile
- median
- mean
- average
- std
- var

## NumPy排序

```python
a = np.array([[4,3,2],[2,4,1]])
print(np.sort(a))
print(np.sort(a, axis=None))
print(np.sort(a, axis=0))
print(np.sort(a, axis=1))
np.sort(a, axis=-1, kind=‘quicksort’, order=None)
```

可选参数：

kind: quicksort(default)、mergesort、heapsort

axis: 

- -1(default) 即沿着数组的最后一个轴进行排序
- None 即采用扁平化的方式作为一个向量进行排序
- 0 跨行（就是按列）
- 1 跨列（就是按行）

order: 结构化的数组可以指定按照某个字段进行排序

# 课时5

## Pandas的两大数据结构

### Series

直观来看，Series是数据表的一列。官方说法，Series是一个定长的字典序列，因为在存储的时候，相当于两个 ndarray，而字典的结构里，元素的个数是不固定的，这也是和字典结构最大的不同。

Series有两个基本属性，index和values（index默认是0，1，2……如果指定索引名就为指定的名字）

### DataFrame

它包括了行索引和列索引，我们可以将 DataFrame 看成是由相同索引的 Series 组成的字典类型。

## 常见数据清洗函数

<img src=".\images\image-20230126204020024.png" alt="image-20230126204020024" style="zoom: 80%;" />

```python
# 数据导入导出
score = DataFrame(pd.read_excel('data.xlsx')) # 导入
score.to_excel('data1.xlsx') # 导出

# 数据清洗
df2 = df2.drop(columns=['Chinese']) # 删除指定列
df2 = df2.drop(index=['ZhangFei']) # 删除指定行
df2.rename(columns={'Chinese': 'YuWen', 'English': 'Yingyu'}, inplace = True) # 重命名
df = df.drop_duplicates() # 去除重复行
df2['Chinese'].astype('str') # 换数据类型
df2['Chinese']=df2['Chinese'].map(str.strip) # 删除左右两边空格
df2['Chinese']=df2['Chinese'].map(str.lstrip) # 删除左边空格
df2['Chinese']=df2['Chinese'].map(str.rstrip) # 删除右边空格
df2['Chinese']=df2['Chinese'].str.strip('$') # 删除指定字符
df2.columns = df2.columns.str.upper() # 全部大写
df2.columns = df2.columns.str.lower() # 全部小写
df2.columns = df2.columns.str.title() # 首字母大写
df.isnull().any() # 查看哪些列有空值

# apply函数
def plus(df,n,m):
 df['new1'] = (df[u'语文']+df[u'英语']) * m
 df['new2'] = (df[u'语文']+df[u'英语']) * n
 return df
df1 = df1.apply(plus,axis=1,args=(2,3,))
'''
其中 axis=1 代表按照列为轴进行操作，axis=0 代表按照行为轴进行操作，args 是传递的
两个参数，即 n=2, m=3，在 plus 函数中使用到了 n 和 m，从而生成新的 df。
'''
```

## 常用统计函数

1. `count()` 统计个数 NaN 和 None 除外
2. `describe()` 一次性输出多个统计指标
3. `min()` 最小值
4. `max()` 最大值
5. `sum()` 求和
6. `mean()` 平均值
7. `median()` 中位数
8. `var()` 方差
9. `std()` 标准差
10. `argmin()` 统计最小值的索引位置
11. `argmax()` 统计最大值的索引位置
12. `idxmin()` 统计最小值的索引值
13. `idxmax()` 统计最大值的索引值

## 合并

```python
df3 = pd.merge(df1, df2, on='name') # 基于指定列合并
df3 = pd.merge(df1, df2, how='inner') # 内连接
df3 = pd.merge(df1, df2, how='left') # 左连接
df3 = pd.merge(df1, df2, how='right') # 右连接
df3 = pd.merge(df1, df2, how='outer') # 外连接
```

## PandaSql

```python
# 单个全局化
global df1
df1 = DataFrame({'name': ['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1': range(5)})
query = """select * from df1 where name ='ZhangFei'"""
print(sqldf(query))

# 一起全局化
df1 = DataFrame({'name': ['ZhangFei', 'GuanYu', 'a', 'b', 'c'], 'data1': range(5)})
pysqldf = lambda sql: sqldf(sql, globals())
sql = "select * from df1 where name ='ZhangFei'"
print(pysqldf(sql))

# lambda argument_list: expression
```

# 补充知识

## DataFrame切片

```python
# 创建DataFrame
df = pd.DataFrame(np.arange(12, 60).reshape((12, 4)), columns=["WW", "XX", "YY", "ZZ"])
'''
    WW  XX  YY  ZZ
0   12  13  14  15
1   16  17  18  19
2   20  21  22  23
3   24  25  26  27
4   28  29  30  31
5   32  33  34  35
6   36  37  38  39
7   40  41  42  43
8   44  45  46  47
9   48  49  50  51
10  52  53  54  55
11  56  57  58  59
'''

# 取行：方括号里写数字
print(df[:5])  # 取0-4行
print(df[2:6]) # 取2-5行

# 取列：方括号里写字符串
print(df["YY"])
print(df["XX", "ZZ"])

# 组合取
print(df[2:6]["YY"])
```

1. `df.loc`通过索引标签名取数据
2. `df.iloc`通过位置获取数据 (位置从0开始算)

`loc()`和`iloc()`并不会拷贝，只是视图上的操作，通过`loc()`或`iloc()`进行赋值修改会直接修改原先的DataFrame

```python
# 创建DataFrame
df = pd.DataFrame(np.arange(12, 32).reshape((5, 4)), index=["a","b","c","d","e"], columns=["WW","XX","YY","ZZ"])
'''
   WW  XX  YY  ZZ
a  12  13  14  15
b  16  17  18  19
c  20  21  22  23
d  24  25  26  27
e  28  29  30  31
'''
# 取一行
print(df.loc["c"])  # Series类型
# 取多行
print(df.loc["a", "d"]) # DataFrame类型
# 取连续多行
print(df.loc["b": "d"]) # 左右都是闭区间
# 取单行单列
print(df.loc["b", "YY"])
# 取单行多列
print(df.loc["b", ["XX", "ZZ"]])  # Series类型
# 取多行多列
print(df.loc[["b", "d"], ["XX", "ZZ"]])
# 取连续多行多列
print(df.loc["b": "d", ["XX", "ZZ"]])
# 取单列所有行
print(df.loc[:, ["XX"]]) # DataFrame类型
print(df.log[:, "XX"]) # Series类型
# 用iloc
print(df.iloc[1:3,[2,3]]) # 注意：切片位置从0开始算左闭右开，方括号两边闭
```

## 数据清洗理论

### 什么是整齐的数据

每一行代表一个观测值，每一列代表一个变量，每一个表格代表一个观测单元

### 混乱数据的特点

1. 列名是值而不是变量
2. 多个变量存储在一列中
3. 变量既存储在列中，也存储在行中
4. 多种类型的观测单元存储在一个表中
5. 单个观测单元存储在多个表中

### 第一类问题：列名是值而不是变量

<img src=".\images\image-20230127154335467.png" alt="image-20230127154335467" style="zoom: 67%;" />

上述数据属于**宽数据**，可以在较小的空间内存储非常多的信息，但数据分析师需要的是**长数据**，需要改成一列表示范围，一列是具体的值。

```python
pd.melt(df, id_vars='religion', var_name='income', value_name='count')

"""
Parameters:
id_vars: 指定哪些列不需要改变
value_vars: 指定哪些列需要分解
var_name: 在分解数据框可以改变默认的列名
value_name: 分解数据后可以重命名值列
"""
```

<img src=".\images\image-20230127155558738.png" alt="image-20230127155558738" style="zoom:67%;" />

### 第二类问题：多个变量存储在一列中

<img src=".\images\image-20230127173204509.png" alt="image-20230127173204509" style="zoom:80%;" />

如上图，病例数、死亡数和国家连在一起，我们需要分离出这些变量

```python
# 分解数据
ebola_long = pd.melt(ebola, id_vars=['Date', 'Day'])
```

<img src=".\images\image-20230127173455407.png" alt="image-20230127173455407" style="zoom:67%;" />

#### 方法1

```python
# 对variable列进行字符串解析成列表
variable_split = ebola_long.variable.str.split('_')
status_value = variable_split.str.get(0)
country_value = variable_split.str.get(1)
ebola_long['status'] = status_value
ebola_long['country'] = country_value
```

<img src=".\images\image-20230127174218944.png" alt="image-20230127174218944" style="zoom:67%;" />

#### 方法2

```python
# 对variable列进行字符串解析成DataFrame
variable_split = ebola_long.variable.str.split('_', expand=True)
# 合并两个数据框的内容
ebola_parsed = pd.concat([ebola_long, variable_split], axis=1)
```

<img src=".\images\image-20230127174443321.png" alt="image-20230127174443321" style="zoom:67%;" />

<img src=".\images\image-20230127175040382.png" alt="image-20230127175040382" style="zoom:80%;" />

#### 方法3

```python
# 合并两个列表zip
variable_split = ebola_long.variable.str.split('_')
# 将序列拆开得到两列，一次性添加到原DataFrame中
ebola_long['status_zip'], ebola_long['country_zip'] = zip(*variable_split)
```

<img src=".\images\image-20230127175736893.png" alt="image-20230127175736893" style="zoom:67%;" />

<img src=".\images\image-20230127180002222.png" alt="image-20230127180002222" style="zoom:67%;" />

### 第三类问题：变量既存储在列中，也存储在行中

<img src=".\images\image-20230127180140877.png" alt="image-20230127180140877" style="zoom:67%;" />

如上图，element列保存的也是变量，即最大值和最小值

```python
# 获取月中的某天并分解它
weather_melt = pd.melt(weather, id_vars=['id', 'year', 'month', 'element'], var_name='day', value_name='temp')

```

<img src=".\images\image-20230127204536172.png" alt="image-20230127204536172" style="zoom:67%;" />

反向操作melt命令将使用数据透视表，可以用于获取某列以及获取该列的每个独立值，并将其转换成单独的一列

在这里我们获取element列并把两个变量填充到图表的列上

```python
weather_tidy = weather_melt.pivot_table(index=['id', 'year', 'month', 'day'], columns='element', values='temp')

"""
pd.pivot_table()
Parameters:
index: 指定哪些列不需要改变
columns: 指定哪些列撤销melt命令
values: 告诉函数用什么值来填充单元格
"""
```

<img src=".\images\image-20230127210208897.png" alt="image-20230127210208897" style="zoom:67%;" />

```python
weather_tidy_flat = weather_tidy.reset_index()
```

<img src=".\images\image-20230127210558389.png" alt="image-20230127210558389" style="zoom:67%;" />

遇到第三类问题，首先需要对其中的某些列进行分解，然后对其中的某一列做数据透视表，最后为了得到一个标准的矩形数据框，要重新设置所有的索引。

### 第四类问题：多种类型的观测单元存储在一个表中

当数据集中有许多重复的信息时就是数据规范化问题，在行政类、教育类数据集中常会遇到这样的问题

<img src=".\images\image-20230127211054393.png" alt="image-20230127211054393" style="zoom:67%;" />

如上图，year/artist/track/time/date.entered都是曲目信息，week/rating是评分信息，需要分解成两个数据框billboard_songs和billboard_ratings

```python
billboard_songs = billboard_long[['year', 'artist', 'track', 'time']]
billboard_songs = billboard_songs.drop_duplicates() # 去掉了评分信息后很多行都是重复的

# 为每一首歌分配一个唯一标识
billboard_songs['id'] = range(len(billboard_songs))

# 与原数据集连接后再去子集
billboard_ratings = billboard_long.merge(billboard_songs, on=['year', 'artist', 'track', 'time'])
billboard_ratings = billboard_ratings[['id', 'date.entered', 'week', 'rating']]
```

### 第五类问题：单个观测单元存储在多个表中

```python
# 需要同时载入多个文件
import glob
concat_files = glob.glob('../data/concat*')

# 得到数据框的列表
list_concat_df = []
for csv_filename in concat_files:
    df = pd.read_csv(csv_filename)
    list_concat_df.append(df)
    
concat_loop = pd.concat(list_concat_df)

```

<img src=".\images\image-20230127215142717.png" alt="image-20230127215142717" style="zoom:67%;" />

# 课时6

## BI、DW、DM三者关系

BI是商业智能，DW是数据仓库，DM是数据挖掘。形象的比喻是，数据仓库是个金矿，数据挖掘是炼金术，商业报告是黄金。

### BI

基于数据仓库，经过数据挖掘后得到的具有商业价值的信息的过程。

### DW

BI的地基，数据仓库是数据库的升级概念，不同的地方在于数据仓库更庞大，使用与数据挖掘和数据分析，而数据库更偏向于一种存储技术。数据仓库将原有的多个数据来源中的数据进行汇总、整理而得。数据进入数据仓库前，必须消除数据中的不一致性，方便后续进行数据分析和挖掘。

### DM

核心包括分类、聚类、预测、关联分析等任务，有了这些数据挖掘技术的支持，才能指导业务层做更好的BI分析。

其他概念：ETL数据搬运工，DM也通常叫做数据科学家。

## 元数据与数据元

元数据（MetaData）：描述其它数据的数据，也称为“中介数据”。
数据元（Data Element）：就是最小数据单元。

在生活中，只要有一类事物，就可以定义一套元数据。元数据可以很方便地应用于数据仓库。比如数据仓库中有数据和数据之间的各种复杂关系，为了描述这些关系，元数据可以对数据仓库的数据进行定义，刻画数据的抽取和转换规则，存储与数据仓库主题有关的各种信息。而且整个数据仓库的运行都是基于元数据的，比如抽取调度数据、获取历史数据等。

通过元数据，可以很方便地帮助我们管理数据仓库。

## KDD的全过程

<img src=".\images\image-20230128203656390.png" alt="image-20230128203656390" style="zoom:67%;" />

### 数据预处理

#### 数据清洗

主要是为了去除重复数据，去噪声（即干扰数据）以及填充缺失值。

#### 数据集成

是将多个数据源中的数据存放在一个统一的数据存储中。

#### 数据变化

就是将数据转换成适合数据挖掘的形式。比如，通过归一化将属性数据按照比例缩放，这样
就可以将数值落入一个特定的区间内，比如 0~1 之间。

### 数据后处理

是将模型预测的结果进一步处理后，再导出。比如在二分类问题中，一般能得到
的是 0~1 之间的概率值，此时把数据以 0.5 为界限进行四舍五入就可以实现后处理。

# 课时7

## 用户画像建模

### 统一化

用户画像的核心就是用户唯一标识，它把“从用户开始使用 APP到下单到售后整个所有的用户行为”进行串联，这样就可以更好地去跟踪和分析一个用户的特征。

### 标签化

为了保证用户画像的全面性，设计标签的维度共有四个：**用户消费行为分析**。

#### 用户标签

性别、年龄、地域、收入、学历、职业等

#### 消费标签

消费习惯、购买意向、是否对促销敏感

#### 行为标签

时间段、频次、时长、访问路径

#### 内容分析

对用户平时浏览的内容，尤其是停留时间长、浏览次数多的内容进行分析，分析出用户对哪些内容感兴趣

<img src=".\images\image-20230128213928477.png" alt="image-20230128213928477"  />

**数据层**指的是用户消费行为里的标签。我们可以打上“事实标签”，作为数据客观的记录。
**算法层**指的是透过这些行为算出的用户建模。我们可以打上“模型标签”，作为用户画像的分类标识。
**业务层**指的是获客、粘客、留客的手段。我们可以打上“预测标签”，作为业务关联的结果。

### 业务化

获得准确全面的用户画像是产品获客（如何进行拉新，通过更精准的营销获取客户）、粘客（个性化推荐，搜索排序，场景运营等）、留客（流失率预测，分析关键节点降低流失率）的保障。

## 案例：美团用户画像

### 第一步：用户唯一标识的统一化

美团已经和大众点评进行了合并，因此在大众点评和美团外卖上都可以进行外卖下单。我们看到，美团采用的是手机号、微信、微博、美团账号的登录方式。大众点评采用的是手机号、微信、QQ、微博的登录方式。这里面两个 APP 共同的登录方式都是手机号、微信和微博。那么究竟哪个可以作为用户的唯一标识呢？当然主要是以用户的注册手机号为标准。这样美团和大众点评的账号体系就可以相通。

在集团内部，各部门之间的协作，尤其是用户数据打通是非常困难的，所以这里建议，如果希望大数据对各个部门都能赋能，一定要在集团的战略高度上，尽早就在最开始的顶层架构上，将用户标识进行统一，这样在后续过程中才能实现用户数据的打通。

### 第二步：标签化打造用户画像

1. 用户标签：性别、年龄、家乡、居住地、收货地址、婚姻、宝宝信息、通过何种渠道进
行的注册。
2. 消费标签：餐饮口味、消费均价、团购等级、预定使用等级、排队使用等级、外卖等
级。
3. 行为标签：点外卖时间段、使用频次、平均点餐用时、访问路径。
4. 内容分析：基于用户平时浏览的内容进行统计，包括餐饮口味、优惠敏感度等。

### 第三步：结合具体业务生成方案

在获客上，我们可以找到优势的宣传渠道，如何通过个性化的宣传手段，吸引有潜在需求的用户，并刺激其转化。
在粘客上，如何提升用户的单价和消费频次，方法可以包括购买后的个性化推荐、针对优质用户进行优质高价商品的推荐、以及重复购买，比如通过红包、优惠等方式激励对优惠敏感的人群，提升购买频次。
在留客上，预测用户是否可能会从平台上流失。在营销领域，关于用户留存有一个观点——如果将顾客流失率降低 5%，公司利润将提升 25%~85%。可以看出留存率是多么的重要。用户流失可能会包括多种情况，比如用户体验、竞争对手、需求变化等，通过预测用户的流失率可以大幅降低用户留存的运营成本。

# 课时8

## 数据源有哪些

<img src=".\images\image-20230128220257975.png" alt="image-20230128220257975" style="zoom: 50%;" />

## 爬虫抓取的方法

在 Python 爬虫中，基本上会经历三个过程。
1. 使用 Requests 爬取内容。我们可以使用 Requests 库来抓取网页信息。Requests 库可以说是 Python 爬虫的利器，也就是 Python 的 HTTP 库，通过这个库爬取网页中的数据，非常方便，可以帮我们节约大量的时间。
2. 使用 XPath 解析内容。XPath 是 XML Path 的缩写，也就是 XML 路径语言。它是一种用来确定 XML 文档中某部分位置的语言，在开发中经常用来当作小型查询语言。XPath可以通过元素和属性进行位置索引。
3. 使用 Pandas 保存数据。Pandas 是让数据分析工作变得更加简单的高级数据结构，我们可以用 Pandas 保存爬取的数据。最后通过 Pandas 再写入到 XLS 或者 MySQL 等数据库中。

Requests、XPath、Pandas 是 Python 的三个利器。当然做 Python 爬虫还有很多利器，比如 Selenium，PhantomJS，或者用 Puppteteer 这种无头模式。

第三方工具中，比较推荐使用八爪鱼，拥有云采集功能。八爪鱼一共有 5000 台服务器，通过云端多节点并发采集，采集速度远远超过本地采集。此外还可以自动切换多个 IP，避免 IP 被封，影响采集。

## 日志采集

日志采集是运维人员的工作之一。日志就是日记的意思，它记录了用户访问网站的全过程：哪些人在什么时间，通过什么渠道（比如搜索引擎、网址输入）来过，都执行了哪些操作；系统是否产生了错误；甚至包括用户的 IP、HTTP 请求的时间，用户代理等。这些日志数据可以被写在一个日志文件中，也可以分成不同的日志文件，比如访问日志、错误日志等。

### 日志采集作用

日志采集最大的作用，就是通过分析用户访问情况，提升系统的性能，从而提高系统承载量。及时发现系统承载瓶颈，也可以方便技术人员基于用户实际的访问情况进行优化。

### 日志采集分类

1. 通过 Web 服务器采集，例如 httpd、Nginx、Tomcat 都自带日志记录功能。同时很多互联网企业都有自己的海量数据采集工具，多用于系统日志采集，如 Hadoop 的Chukwa、Cloudera 的 Flume、Facebook 的 Scribe 等，这些工具均采用分布式架构，能够满足每秒数百 MB 的日志数据采集和传输需求。
2. 自定义采集用户行为，例如用 JavaScript 代码监听用户的行为、AJAX 异步请求后台记录日志等。

### 埋点

在有需要的位置采集相应的信息，进行上报。比如某页面的访问情况，包括用户信
息、设备信息；或者用户在页面上的操作行为，包括时间长短等。这就是埋点，每一个埋点就像一台摄像头，采集用户行为数据，将数据进行多维度的交叉分析，可真实还原出用户使用场景，和用户使用需求。

埋点就是在你需要统计数据的地方植入统计代码，当然植入代码可以自己写，也可以使用第三方统计工具。

这里推荐你使用第三方的工具，比如友盟、Google Analysis、Talkingdata 等。他们都是采用前端埋点的方式，然后在第三方工具里就可以看到用户的行为数据。但如果我们想要看到更深层的用户操作行为，就需要进行自定义埋点。

# 课时9

## 八爪鱼爬取数据流程

<img src=".\images\image-20230129104421668.png" alt="image-20230129104421668" style="zoom:67%;" />

## 八爪鱼两大工具

### 流程视图

流程视图应该是在可视化中应用最多的场景，我们可以使用流程视图查看创建流程，调整顺序，或者删掉不想要的步骤。

另外我们还能在视图中查看数据提取的字段。选中“提取数据”步骤，可以看到该步骤提取的字段都有哪些。

### XPath

XPath全称是XML Path Language，也就是 XML 的路径语言，用来在 XML 文件中寻找我们想要的元素。

八爪鱼工具中内置了 XPath 引擎，所以在我们用可视化方式选择元素的时候，会自动生成相应的 XPath 路径。当然我们也可以查看这些元素的 XPath，方便对它们进行精细地控制。这是因为我们采集的网站页面是不规律的，有时候翻页后的排版是不同的样的话，可视化操作得到的 XPath 可能不具备通用性。

![image-20230129105944928](.\images\image-20230129105944928.png)

# 课时10

## 爬虫的流程

1. 打开网页：可以使用 Requests 访问页面，得到服务器返回给我们的数据，这里包括 HTML 页面以及 JSON 数据。
2. 提取数据：针对 HTML 页面，可以使用 XPath 进行元素定位，提取数据；针对 JSON 数据，可以使用 JSON 进行解析。
3. 保存数据：Pandas保存数据最后到处CSV文件。

## XPath定位

XPath是XML的路径语言，帮助我们对元素和属性进行导航定位。

| 表达式 | 含义                                 |
| ------ | ------------------------------------ |
| node   | 选node节点的所有子节点               |
| /      | 从根节点选取                         |
| //     | 选取所有的当前节点，不考虑他们的位置 |
| .      | 当前节点                             |
| ..     | 父节点                               |
| @      | 属性选择                             |
| \|     | 或，两个节点的合计                   |
| text() | 当前路径下的文本内容                 |

1. xpath(‘node’) 选取了 node 节点的所有子节点；
2. xpath(’/div’) 从根节点上选取 div 节点；
3. xpath(’//div’) 选取所有的 div 节点；
4. xpath(’./div’) 选取当前节点下的 div 节点；
5. xpath(’…’) 回到上一个节点；
6. xpath(’//@id’) 选取所有的 id 属性；
7. xpath(’//book[@id]’) 选取所有拥有名为 id 的属性的 book 元素；
8. xpath(’//book[@id=“abc”]’) 选取所有 book 元素，且这些 book 元素拥有 id="abc"的属性；
9. xpath(’//book/title | //book/price’) 选取 book 元素的所有 title 和 price 元素。

使用 XPath 定位，你会用到 Python 的一个解析库 lxml。比如我们想要定位到 HTML 中的所有列表项目，可以采用下面这段代码。

```python
from lxml import etree
html = etree.HTML(html)
result = html.xpath('//li')
```

## JSON对象

JSON是一种轻量级的交互方式，在 Python 中有 JSON 库，可以让我们将 Python 对象和 JSON 对象进行转换。

```python
import json
jsonData = '{"a":1,"b":2,"c":3,"d":4,"e":5}'
pyObj = json.loads(jsonData)
jsonObj = json.dumps(pyObj)
```

## Python爬虫的两种方案

### JSON

<img src=".\images\image-20230129170914331.png" alt="image-20230129170914331" style="zoom: 67%;" />

这里你需要注意的是，如果爬取的页面是动态页面，就需要关注 XHR 数据。因为动态页面的原理就是通过原生的 XHR 数据对象发出 HTTP 请求，得到服务器返回的数据后，再进行处理。XHR 会用于在后台与服务器交换数据。

如果看到的是类似下图这么清爽的JSON数据，那么直接转换对象，依次调用下载函数即可。

<img src=".\images\image-20230129171038370.png" alt="image-20230129171038370" style="zoom: 67%;" />

### XPath

有时候，网页会用 JS 请求数据，那么只有 JS 都加载完之后，我们才能获取完整的HTML 文件。XPath 可以不受加载的限制，帮我们定位想要的元素。

```python
srcs = html.xpath(src_xpath)
titles = html.xpath(title_path)
for src, title in zip(srcs, titles):
	download(src, title.text)
```

有时候当我们直接用 Requests 获取 HTML 的时候，发现想要的 XPath 并不存在。这是因为 HTML 还没有加载完，因此你需要一个工具，来进行网页加载的模拟，直到完成加载后再给你完整的 HTML。在 Python 中，这个工具就是 Selenium 库，使用方法如下：

```python
from selenium import webdriver
driver = webdriver.Chrome()
driver.get(request_url)
```

`Selenium` 是 Web 应用的测试工具，可以直接运行在浏览器中，它的原理是模拟用户在进行操作，支持当前多种主流的浏览器。

然后通过 `WebDriver` 创建一个 Chrome 浏览器的 drive，再通过 drive 获取访问页面的完整 HTML。

> Python + Selenium + 第三方浏览器可以让我们处理多种复杂场景，包括网页动态加载、JS 响应、Post 表单等。因为 Selenium 模拟的就是一个真实的用户的操作行为，就不用担心 cookie 追踪和隐藏字段的干扰了。

## Python爬虫工具

`Scrapy` 是一个 Python 的爬虫框架，它依赖的工具比较多，所以在 pip install 的时候，会安装多个工具包。`scrapy` 本身包括了爬取、处理、存储等工具。在 `scrapy` 中，有一些组件是提供给你的，需要你针对具体任务进行编写。比如在 `item.py` 对抓取的内容进行定义，在 `spider.py` 中编写爬虫，在 `pipeline.py` 中对抓取的内容进行存储，可以保存为 `csv`等格式。

`Puppeteer` 是个很好的选择，可以控制 Headless Chrome，这样就不用 `Selenium`和 `PhantomJS`。与 `Selenium` 相比，`Puppeteer` 直接调用 Chrome 的 API 接口，不需要打开浏览器，直接在 `V8` 引擎中处理，同时这个组件是由 Google 的 Chrome 团队维护的，所以兼容性会很好。

# 课时11

## 数据质量的准则

1. 完整性：单条数据是否存在空值，统计的字段是否完善
2. 全面性：观察某一列的全部数值，根据常识判断该列是否有问题，比如数据定义、单位标识、数值本身
3. 合法性：数据是否存在类型（比如非ASCII字符）、内容（性别存在未知）、大小（年龄超过150岁等）的合法性问题
4. 唯一性：数据是否存在重复记录

## 不同问题的数据清洗方法

![image-20230131110456513](.\images\image-20230131110456513.png)

### 完整性问题

即列中出现`NaN`的情况，通常可以通过删除或者均值、高频值替代的方式

```python
df['Age'].fillna(df['Age'].mean(), inplace=True) # 平均年龄填充

age_maxf = train_features['Age'].value_counts().index[0]
train_features['Age'].fillna(age_maxf, inplace=True) # 最高频值填充
```

数据行中除了index之外都是`NaN`，则为空行。读入的时候无法忽略，需要读入后处理

```python
# 删除全空的行
df.dropna(how='all', inplace=True )
```

## 全面性问题

weight列的数值单位不统一

```python
rows_with_lbs = df['weight'].str.contains('lbs').fillna(False) # 获取单位为lbs的数据

# 将lbs转换为kgs，2.2lbs=1kgs
# 遍历所有含lbs的行
for i,lbs_row in df[rows_with_lbs].iterrows():
    # 去掉lbs
    weight = int(float(lbs_row['weight'][:-3]) / 2.2)
    df.at[i, 'weight'] = '{}kgs'.format(weight)

```

## 合理性问题

`FirstName`和`LastName`中有一些非ASCII字符，可以采用删除或替换的方式来解决

```python
df['first_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
df['last_name'].replace({r'[^\x00-\x7F]+':''}, regex=True, inplace=True)
```

## 唯一性问题

一列中有多个参数，如这里Name包含了`FirstName`和`LastName`两个字段

```python
df[['first_name', 'last_name']] = df['name'].split(expand=True)
df.drop('name', axis=1, inplace=True)
```

数据中如果存在重复记录，就需要删除

```python
df.drop_duplicates(['first_name', 'last_name'], inplace=True)
```

![image-20230131111934067](.\images\image-20230131111934067.png)

# 课时12

## 数据集成

这里数据集成的含义包括了数据清洗、数据抽取、数据集成、数据变换等操作，是一系列数据挖掘前的准备工作，为的是将数据源的数据统一到数据仓库中，并保证数据的干净清爽。

## 数据集成的两种架构

### `ETL`

`ETL`是Extract、Transform 和 Load 的缩写，包括了数据的抽取、转换、加载这三个过程

抽取：将数据从数据源中提取出来

转换：对原始数据进行处理，比如连接等操作

加载：将转换的结果写入目的地

### `ELT`

和`ETL`区别在于`ELT`顺序不同，先抽取后将结果先写入目的地，然后利用数据库的聚合分析能力或外部计算框架（如Spark）完成转换的步骤

### 比较

目前数据集成的主流架构是 `ETL`，但未来使用 `ELT`作为数据集成架构的将越来越多。这样做会带来多种好处：

1. `ELT` 和 `ETL` 相比，最大的区别是“重抽取和加载，轻转换”，从而可以用更轻量的方案搭建起一个数据集成平台。使用 `ELT` 方法，在提取完成之后，数据加载会立即开始。一方面更省时，另一方面 `ELT` 允许 `BI` 分析人员无限制地访问整个原始数据，为分析师提供了更大的灵活性，使之能更好地支持业务。
2. 在 `ELT` 架构中，数据变换这个过程根据后续使用的情况，需要在 SQL 中进行，而不是在加载阶段进行。这样做的好处是你可以从数据源中提取数据，经过少量预处理后进行加载。这样的架构更简单，使分析人员更好地了解原始数据的变换过程。

### `ETL`主流工具

#### `Kettle`

Kettle 是一款国外开源的 ETL 工具，纯 Java 编写，可以在 Window 和 Linux 上运行，不需要安装就可以使用。Kettle 中文名称叫水壶，该项目的目标是将各种数据放到一个壶里，然后以一种指定的格式流出。

Kettle 采用可视化的方式进行操作，来对数据库间的数据进行迁移。它包括了两种脚本：Transformation 转换和 Job 作业。

Transformation（转换）：相当于一个容器，对数据操作进行了定义。数据操作就是数据从输入到输出的一个过程。
Job（作业）：相比于转换是个更大的容器，它负责将转换组织起来完成某项作业。在通常的工作中，我们会把任务分解成为不同的作业，然后再把作业分解成多个转换。

#### `DataX`

数据库之间没有统一标准，转换时都是两两之间进行

<img src=".\images\image-20230131114007220.png" alt="image-20230131114007220" style="zoom:67%;" />

`DataX`可以实现跨平台、跨数据库、不同系统之间的数据同步及交互，它将自己作为标准，连接了不同的数据源，以完成它们之间的转换。

<img src=".\images\image-20230131114109718.png" alt="image-20230131114109718" style="zoom:67%;" />

`DataX`的模式是基于框架+插件完成的。`DataX`的框架如下图：

<img src=".\images\image-20230131114246264.png" alt="image-20230131114246264" style="zoom: 80%;" />

在这个框架里，Job 作业被 Splitter 分割器分成了许多小作业 Sub-Job。在 `DataX`里，通过两个线程缓冲池来完成读和写的操作，读和写都是通过 Storage 完成数据的交换。比如在“读”模块，切分后的小作业，将数据从源头装载到 `DataXStorage`，然后在“写”模块，数据从 `DataXStorage` 导入到目的地。

这样的好处就是，在整体的框架下，我们可以对 Reader 和 Writer 进行插件扩充，比如我想从 MySQL 导入到 Oracle，就可以使用 MySQLReader 和 OracleWriter 插件，装在框架上使用即可。

#### `Sqoop`

`Sqoop` 是一款开源的工具，是由 Apache 基金会所开发的分布式系统基础架构。`Sqoop`在 `Hadoop` 生态系统中是占据一席之地的，它主要用来在 `Hadoop` 和关系型数据库中传递数据。通过 `Sqoop`，我们可以方便地将数据从关系型数据库导入到 `HDFS` 中，或者将数据从 `HDFS` 导出到关系型数据库中。

`Hadoop` 实现了一个分布式文件系统，即 `HDFS`。`Hadoop` 的框架最核心的设计就是`HDFS` 和 `MapReduce`。`HDFS` 为海量的数据提供了存储，而 `MapReduce` 则为海量的数据提供了计算。

# 课时13

## 数据准备流程

![image-20230131115047955](.\images\image-20230131115047955.png)

在数据变换前，我们需要先对字段进行筛选，然后对数据进行探索和相关性分析，接着是选择算法模型（这里暂时不需要进行模型计算），然后针对算法模型对数据的需求进行数据变换，从而完成数据挖掘前的准备工作。

## 数据变换

通过数据平滑、数据聚集、数据概化和规范化等方式将数据转化成统一的形式，方便后续的分析处理。  

1. 数据平滑：去除数据中的噪声，将连续数据离散化。这里可以采用分箱、聚类和回归的方式进行数据平滑
2. 数据聚集：对数据进行汇总，在 SQL 中有一些聚集函数可以供我们操作，比如 Max() 反馈某个字段的数值最大值，Sum() 返回某个字段的数值总和
3. 数据概化：将数据由较低的概念抽象成为较高的概念，减少数据复杂度，即用更高的概念替代更低的概念。比如说上海、杭州、深圳、北京可以概化为中国
4. 数据规范化：使属性数据按比例缩放，这样就将原来的数值映射到一个新的特定区域中。常用的方法有最小—最大规范化、Z—score 规范化、按小数定标规范化等
5. 属性构造：构造出新的属性并添加到属性集中。这里会用到特征工程的知识，因为通过属性与属性的连接构造新的属性，其实就是特征工程。比如说，数据表中统计每个人的英语、语文和数学成绩，你可以构造一个“总和”这个属性，来作为新属性。这样“总和”这个属性就可以用到后续的数据挖掘计算中。

## 数据规范化方法

1. Min-max 规范化
  Min-max 规范化方法是将原始数据变换到 [0,1] 的空间中。用公式表示就是：新数值 =（原数值 - 极小值）/（极大值 - 极小值）。

  ```python
  # SciKit-Learn 不仅可以用于数据变换，它还提供了分类、聚类、预测等数据挖掘算法的 API 封装。
  from sklearn import preprocessing
  import numpy as np
  # 初始化数据，每一行表示一个样本，每一列表示一个特征
  x = np.array([[ 0., -3., 1.],
   [ 3., 1., 2.],
   [ 0., 1., -1.]])
  # 将数据进行 [0,1] 规范化
  min_max_scaler = preprocessing.MinMaxScaler()
  minmax_x = min_max_scaler.fit_transform(x)
  print(minmax_x)
  ```

2. Z-Score 规范化

   新数值 =（原数值 - 均值）/ 标准差。Z-Score 的优点是算法简单，不受数据量级影响，结果易于比较。不足在于，它需要数据整体的平均值和方差，而且结果没有实际意义，只是用于比较。

   ```python
   # 将数据进行 Z-Score 规范化
   scaled_x = preprocessing.scale(x)
   print(scaled_x)
   ```

3. 小数定标规范化
   小数定标规范化就是通过移动小数点的位置来进行规范化。小数点移动多少位取决于属性 A的取值中的最大绝对值。
   举个例子，比如属性 A 的取值范围是 -999 到 88，那么最大绝对值为 999，小数点就会移动 3 位，即新数值 = 原数值 /1000。那么 A 的取值范围就被规范化为 -0.999 到 0.088。

   ```python
   # 小数定标规范化
   j = np.ceil(np.log10(np.max(abs(x))))
   scaled_x = x/(10**j)
   print(scaled_x)
   ```

# 课时14

## 数据可视化的视图分类

- 分布
- 时间相关
- 局部/整体
- 偏差
- 相关性
- 排名
- 量级
- 地图
- 流动

## 数据可视化的工具

<img src=".\images\image-20230131163923992.png" alt="image-20230131163923992" style="zoom:67%;" />

# 课时15

## 视图分类

按照数据之间的关系，我们可以把可视化视图划分为 4 类，它们分别是比较、联系、构成和分布。
1. 比较：比较数据间各类别的关系，或者是它们随着时间的变化趋势，比如折线图；
2. 联系：查看两个或两个以上变量之间的关系，比如散点图；
3. 构成：每个部分占整体的百分比，或者是随着时间的百分比变化，比如饼图；
4. 分布：关注单个变量，或者多个变量的分布情况，比如直方图。

按照变量的个数，我们可以把可视化视图划分为单变量分析和多变量分析。

## 散点图

它将两个变量的值显示在二维坐标中，非常适合展示两个变量之间的关系。当然，除了二维的散点图，我们还有三维的散点图。

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 数据准备
N = 1000
x = np.random.randn(N)
y = np.random.randn(N)
# 用 Matplotlib 画散点图
plt.scatter(x, y,marker='x')
plt.show()
# 用 Seaborn 画散点图
df = pd.DataFrame({'x': x, 'y': y})
sns.jointplot(x="x", y="y", data=df, kind='scatter');
plt.show()
```

## 折线图

折线图可以用来表示数据随着时间变化的趋势。

```python
# 数据准备
x = [2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
y = [5, 3, 6, 20, 17, 16, 19, 30, 32, 35]
# 使用 Matplotlib 画折线图
plt.plot(x, y)
plt.show()
# 使用 Seaborn 画折线图
df = pd.DataFrame({'x': x, 'y': y})
sns.lineplot(x="x", y="y", data=df)
plt.show()
```

## 直方图

直方图是比较常见的视图，它是把横坐标等分成了一定数量的小区间，这个小区间也叫作“箱子”，然后在每个“箱子”内用矩形条（bars）展示该箱子的箱子数（也就是 y值），这样就完成了对数据集的直方图分布的可视化。

```python
# 数据准备
a = np.random.randn(100)
s = pd.Series(a)
# 用 Matplotlib 画直方图
plt.hist(s)
plt.show()
# 用 Seaborn 画直方图
sns.distplot(s, kde=False)
plt.show()
sns.distplot(s, kde=True) # 默认值，kde表示核密度
plt.show()
```

## 条形图

条形图可以帮我们查看类别的特征。在条形图中，长条形的长度表示类别的频数，宽度表示类别。

```python
# 数据准备
x = ['Cat1', 'Cat2', 'Cat3', 'Cat4', 'Cat5']
y = [5, 4, 8, 12, 7]
# 用 Matplotlib 画条形图
plt.bar(x, y)
plt.show()
# 用 Seaborn 画条形图
sns.barplot(x, y)
plt.show()
```

## 箱线图

由五个数值点组成：最大值 (max)、最小值(min)、中位数 (median) 和上下四分位数 (Q3, Q1)。它可以帮我们分析出数据的差异性、离散程度和异常值等。

```python
# 数据准备
# 生成 0-1 之间的 10*4 维度数据
data=np.random.normal(size=(10,4)) 
lables = ['A','B','C','D']
# 用 Matplotlib 画箱线图
plt.boxplot(data,labels=lables)
plt.show()
# 用 Seaborn 画箱线图
df = pd.DataFrame(data, columns=lables)
sns.boxplot(data=df)
plt.show()
```

## 饼图

饼图是常用的统计学模块，可以显示每个部分大小与总和之间的比例。

```python
# 数据准备
nums = [25, 37, 33, 37, 6]
labels = ['High-school','Bachelor','Master','Ph.d', 'Others']
# 用 Matplotlib 画饼图
plt.pie(x = nums, labels=labels)
plt.show()
```

## 热力图

热力图是一种非常直观的多元变量分析方法，是一种矩阵表示方法，其中矩阵中的元素值用颜色来代表，不同的颜色代表不同大小的值。通过颜色就能直观地知道某个位置上数值的大小。另外你也可以将这个位置上的颜色，与数据集中的其他位置颜色进行比较。

```python
# 数据准备
flights = sns.load_dataset("flights")
data=flights.pivot('year','month','passengers')
# 用 Seaborn 画热力图
sns.heatmap(data)
plt.show()
```

<img src=".\images\image-20230131165907038.png" alt="image-20230131165907038" style="zoom:67%;" />

## 蜘蛛图

蜘蛛图是一种显示一对多关系的方法。在蜘蛛图中，一个变量相对于另一个变量的显著性是清晰可见的。

```python
from matplotlib.font_manager import FontProperties
# 数据准备
labels=np.array([u" 推进 ","KDA",u" 生存 ",u" 团战 ",u" 发育 ",u" 输出 "])
stats=[83, 61, 95, 67, 76, 88]
# 画图数据准备，角度、状态值
angles=np.linspace(0, 2*np.pi, len(labels), endpoint=False)
stats=np.concatenate((stats,[stats[0]]))
angles=np.concatenate((angles,[angles[0]]))
# 用 Matplotlib 画蜘蛛图
fig = plt.figure()
ax = fig.add_subplot(111, polar=True) 
ax.plot(angles, stats, 'o-', linewidth=2)
ax.fill(angles, stats, alpha=0.25)
# 设置中文字体
font = FontProperties(fname=r"C:\Windows\Fonts\simhei.ttf", size=14) 
ax.set_thetagrids(angles * 180/np.pi, labels, FontProperties=font)
plt.show()
```

<img src=".\images\image-20230131170254977.png" alt="image-20230131170254977" style="zoom:67%;" />

## 二元变量分布

如果我们想要看两个变量之间的关系，就需要用到二元变量分布。当然二元变量分布有多种呈现方式，开头给你介绍的散点图就是一种二元变量分布。

```python
# 数据准备
tips = sns.load_dataset("tips")
print(tips.head(10))
# 用 Seaborn 画二元变量分布图（散点图，核密度图，Hexbin 图）
sns.jointplot(x="total_bill", y="tip", data=tips, kind='scatter')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='kde')
sns.jointplot(x="total_bill", y="tip", data=tips, kind='hex')
plt.show()
```

## 成对关系

如果想要探索数据集中的多个成对双变量的分布，可以直接采用 sns.pairplot() 函数。它会同时展示出 DataFrame 中每对变量的关系，另外在对角线上，你能看到每个变量自身作为单变量的分布情况。它可以说是探索性分析中的常用函数，可以很快帮我们理解变量对之间的关系。

```python
# 数据准备
iris = sns.load_dataset('iris')
# 用 Seaborn 画成对关系
sns.pairplot(iris)
plt.show()
```

这里我们用 Seaborn 中的 pairplot 函数来对数据集中的多个双变量的关系进行探索，如下图所示。从图上你能看出，一共有 sepal_length、sepal_width、petal_length 和petal_width4 个变量，它们分别是花萼长度、花萼宽度、花瓣长度和花瓣宽度。
下面这张图相当于这 4 个变量两两之间的关系。比如矩阵中的第一张图代表的就是花萼长度自身的分布图，它右侧的这张图代表的是花萼长度与花萼宽度这两个变量之间的关系。

<img src=".\images\image-20230131170942322.png" alt="image-20230131170942322" style="zoom:80%;" />